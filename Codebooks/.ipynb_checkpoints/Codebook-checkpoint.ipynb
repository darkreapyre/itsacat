{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serverless Neural Network Codebook: 2-Layer Sample\n",
    "## Architecture Overview\n",
    "\n",
    "The **“Serverless Neural Network”** is a solution for generating a Cloud Native image classifier, using idempotent Amazon Web Services ([AWS](https://aws.amazon.com/what-is-aws/)) Lambda Functions. The **\"SNN\"** is used to train a model to predict whether a particular image is a **“cat”** vs. **“non-cat”** and fits within an overall prediction pipeline as the model training process. The primary objective of using Lambda Functions as opposed to other services like **SageMaker** or dedicated Machine Leaning Frameworks like **MXNet **or ** TensorFlow**, is to remove the abstraction layer that inevitably perpetuates the concept that Neural Networks or Deep Learning is a “black box” architecture and thus somewhat difficult to understand.\n",
    "\n",
    "<img src=\"images/Prediction_Architecture.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><center>**Machine Learning Pipeline**</center></caption><br>\n",
    "\n",
    "By simulating the individual Neurons and the mathematical functions they perform, it's easier to learn the exactly what each neuron is doing and how they contribute to optimizing (or “learning”) the overall hyper-parameters used for the final prediction model. Additionally, leveraging this framework for model training will hopefully provide a more in-depth understanding of how each neuron deals with the \"vectorized\" matrix calculations during Forward Propagation process **AND** the gradient derivative calculations during the Backward Propagation process.\n",
    "\n",
    "## Neural Network Overview\n",
    "\n",
    "<img src=\"images/2layerNN_kiank.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><left>[*image source](https://www.deeplearning.ai)</left></caption><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neural Network Model (shown above) can be summarized as:\n",
    "  \n",
    "**INPUT --> LINEAR/RELU --> LINEAR/SIGMOID --> OUTPUT**  \n",
    "\n",
    "- The **Input** is a $(64, 64, 3)$ image what is flattened to a vector $(12288, 1)$. See the **Data Overview** Section.\n",
    "- The corresponding vector: $[x_{0}, x_{1}, \\dots, x_{12287}]^T$ is then multiplied by the **weight matrix** $W^{[1]}$ of size $(n^{[1]}, 12288)$.\n",
    "- The **bias** term is then added to take the **Relu** (non-linear activation) to get a vector of size $[a^{[1]}_0, a^{[2]}_1, \\dots, a^{[1]}_{n^{[1]}-1}]^T$.\n",
    "- The process is then repeated for the next layer, by taking the resulting vector and multiplying it by the weight matrix $W^{[2]}$ and then adding the intercept (**bias**).\n",
    "- Lastly, the **sigmoid** activation is applied to the result. If the result is greater then $0.5$, it is classified as a **Cat**.\n",
    "\n",
    "Therefore, the **Network Model Parameters** (*parameters.json*) for the above process are as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"epochs\": 10,\n",
    "    \"layers\": 2,\n",
    "    \"activations\": {\n",
    "        \"layer1\": \"relu\",\n",
    "        \"layer2\": \"sigmoid\"\n",
    "    },\n",
    "    \"neurons\": {\n",
    "        \"layer1\": 3,\n",
    "        \"layer2\": 1\n",
    "    },\n",
    "    \"learning_rate\": 0.0075\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Implementation\n",
    "To implement the Neural Network using the **SNN** framework and the above Network configuration, the workflow is comprised of five key steps:  \n",
    "1. Network Initialization.\n",
    "2. Forward Propagation.\n",
    "3. Calculate the Loss (Cost Function).\n",
    "4. Backward Propagation.\n",
    "5. Parameter Optimization (Gradient Descent).\n",
    "\n",
    "The outcome of the above stages provides the optimal model parameters, for use in final prediction, as can be seen in the process diagram below.  \n",
    "\n",
    "<img src=\"images/final_outline.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><left>[*image source](https://www.deeplearning.ai)</left></caption><br>\n",
    "\n",
    "The next sections will further describe each phase in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Initialization\n",
    "For an **L-Layer** network, the *Weights* and *Bias* must be initialized for each individual layer, therefore the dimensions for these matrices must match to the dimensions of each layer. For example, if $n^{[l]}$ is the number of hidden units (neurons) in layer $l$ and the size of the input $X$ is $(12288, 209)$, for $m = 209$ training examples, then:\n",
    "\n",
    "\n",
    "|               \t|      **Shape of W**      \t|  **Shape of b**  \t|                 **Activation**                \t| **Shape of Activation** \t|\n",
    "|---------------\t|:------------------------:\t|:----------------:\t|:---------------------------------------------:\t|:-----------------------:\t|\n",
    "| **Layer 1**   \t| $(n^{[1]},12288)$        \t| $(n^{[1]},1)$    \t| $Z^{[1]} = W^{[1]},X + b^{[1]}$               \t| $(n^{[1]},209)$         \t|\n",
    "| **Layer 2**   \t| $(n^{[2]}, n^{[1]})$     \t| $(n^{[2]},1)$    \t| $Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$         \t| $(n^{[2]}, 209)$        \t|\n",
    "| $\\vdots$      \t| $\\vdots$                 \t| $\\vdots$         \t| $\\vdots$                                      \t| $\\vdots$                \t|\n",
    "| **Layer L-1** \t| $(n^{[L-1]}, n^{[L-2]})$ \t| $(n^{[L-1]}, 1)$ \t| $Z^{[L-1]} =,W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ \t| $(n^{[L-1]}, 209)$      \t|\n",
    "| **Layer L**   \t| $(n^{[L]}, n^{[L-1]})$   \t| $(n^{[L]}, 1)$   \t| $Z^{[L]} =,W^{[L]} A^{[L-1]} + b^{[L]}$       \t| $(n^{[L]}, 209)$        \t|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compute $W X + b$ in python, it carries out broadcasting. For example, if:  \n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "j  & k  & l \\\\\\\n",
    "m  & n & o  \\\\\\\n",
    "p  & q & r\n",
    "\\end{bmatrix}\n",
    "\\space\n",
    "\\space\n",
    "X = \\begin{bmatrix}\n",
    "a  & b  & c \\\\\\\n",
    "d  & e & f \\\\\\\n",
    "g  & h & i \n",
    "\\end{bmatrix}\n",
    "\\space\n",
    "\\space\n",
    "b =\\begin{bmatrix}\n",
    "s  \\\\\\\n",
    "t  \\\\\\\n",
    "u\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Then $WX + b$ will be:\n",
    "\n",
    "$$\n",
    "WX + b = \\begin{bmatrix}\n",
    "(ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\\\n",
    "(ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\\\n",
    "(pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the initialization process, the *Weights* are initialized randomly using a \"standard\" normal distribution with a mean of $0$ and a standard deviation of $1$. To further constrain the weights to be close to zero **but** not exactly zero (for *symmetry breaking*), each random weight is multiplied by $0.01$. The *Bias* is initialized to zero but also multiplied by $0.01$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "The *Forward Propagation* step of the process is comprised of two separate pieces, the **Linear** activation and the **Non-Linear** activation to constrain the outputs between $0$ and $1$.  \n",
    "\n",
    "#### Linear Activation\n",
    "The *Linear* part of the activation computes the following equation:  \n",
    "$$Z^{[l]} = W^{[l]} \\cdot A^{[l]} + b^{[l]}$$\n",
    "\n",
    ">**Note:** It is important to cache the Linear Activations ($Z$) for later use in he Backward Propagation process.\n",
    "\n",
    "Where $A^{[0]} = X$\n",
    "\n",
    "#### Non-Linear Activation\n",
    "The **L-Layer** Neural Network implements two differnt non-linear activation functions:  \n",
    "\n",
    "- **Rectified Linear Unit (ReLU):** The mathematical formula for the *ReLU* function is $A = ReLU(Z) = max(0, Z)$.  \n",
    "- **Sigmoid:** The methematical formula for the *Sigmoid* function is $\\sigma(Z) = \\sigma(W\\cdot A+b) = \\frac{1}{1 + e^{(-z)}}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "**Cross Entropy** is commonly-used in binary classification (labels are assumed to take values $0$ or $1$) as a loss function which is computed by:\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\big[y^{(i)}\\cdot\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\cdot\\log\\left(1-a^{[L](i)}\\right)\\big]$$\n",
    "\n",
    "Where $a^{[L](i)}$ is the last layer of the network and is synonymous with $\\hat{y}$.\n",
    "\n",
    "Cross entropy measures the divergence between two probability distribution, if the cross entropy is large, which means that the difference between two distribution is large, while if the cross entropy is small, which means that two distribution is similar to each other. Generally, comparing to quadratic cost function, cross entropy cost function has the advantages that fast convergence and is more likely to reach the global optimization. For the mathematical details, see [wikipedia](https://en.wikipedia.org/wiki/Cross_entropy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation\n",
    "*Backward Propagation* is used to calculate the the gradient of the *Loss* function with respect to the various paramaters, as follows:\n",
    "\n",
    "<img src=\"images/backprop_kiank.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><left>[*image source](https://www.deeplearning.ai)</left></caption><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Linear Derivative\n",
    "As with *Forward Propagation*, there are two derivative non-linear activation functions for *Sigmoid* and *ReLU* respectively. If $g(\\cdot)$ is the activations function, then the derivative of *Sigmoid* and *ReLU* compute:\n",
    "$$dZ^{[l]} = \\frac{\\partial\\mathcal{L}}{\\partial Z^{[l]}} = dA^{[l]} \\cdot g^{'}(Z^{[l]})$$\n",
    "\n",
    "#### Linear Derivative\n",
    "Once the derivative of the non-linear activation is computed, the derivatives of $W^{[l]}$, $b^{[l]}$ and $A^{[l]}$, are computed using the input $dZ^{[l]}$, to get , $dW^{[l]}$, $db^{[l]}$, $dA^{[l-1]}$ as follows:\n",
    "\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}$$  \n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$  \n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Update using Gradient Descent\n",
    "The Model parameters ($W^{[l]}$ and $b^{[l]}$) are updated using **Gradient Descent** using the following formula:  \n",
    "\n",
    "$$W^{[l]} = W^{[l]} - \\alpha\\cdot dW^{[l]}$$  \n",
    "$$b^{[l]} = b^{[l]} - \\alpha\\cdot db^{[l]}$$  \n",
    "\n",
    "Where $\\alpha$ is the *Learning Rate*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "After the fitted parameters are updated using *Gradient Descent*, the paramaters can be used to predict wether a new image can classified as a **cat** or **non-cat** image. For further information on how the accuracy of the trained model fairs against testing data or unseen data, see the **Analysis** Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Libraries, Global and Event Variables\n",
    "\n",
    "The cell below imports all the packages that will be needed by the Lambda Function. \n",
    "- [datetime](https://docs.python.org/2/library/datetime.html) provides classes for manipulating dates and times in both simple and complex ways.\n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [h5py](http://www.h5py.org) is a common package to interact with a dataset that is stored on an H5 file.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "- [PIL](http://www.pythonware.com/products/pil/) and [scipy](https://www.scipy.org/) are used here to test your model with your own picture at the end.\n",
    "- [boto3](https://pypi.python.org/pypi/boto3) is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python, which allows Python developers to write software that makes use of services like Amazon S3 and Amazon EC2.\n",
    "- [json](https://docs.python.org/3/library/json.html) is a lightweight data interchange format inspired by JavaScript object literal syntax (although it is not a strict subset of JavaScript.\n",
    "- [os](https://docs.python.org/3/library/os.html) is a module the provides a portable way of using operating system dependent functionality. Particularly the  `environ` object is a mapping object representing the environment.\n",
    "- [uuid](https://docs.python.org/2/library/uuid.html#uuid.uuid4) creates a unique, random ID.\n",
    "- The [io](https://docs.python.org/2/library/io.html) module provides the Python interfaces to stream handling.\n",
    "- The Python interface to the [Redis](https://pypi.python.org/pypi/redis) key-value store.\n",
    "\n",
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries needed by the Lambda Function\n",
    "import datetime\n",
    "import numpy as np\n",
    "import h5py\n",
    "import scipy\n",
    "import os\n",
    "from os import environ\n",
    "import json\n",
    "from json import dumps, loads\n",
    "from boto3 import client, resource, Session\n",
    "import botocore\n",
    "import uuid\n",
    "import io\n",
    "import redis\n",
    "from redis import StrictRedis as redis\n",
    "\n",
    "# Import libraries needed for the Codebook\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 Trigger Event\n",
    "T0 initiate the network training process, the dataset (**datasets.h5**) is uploaded to Amazon Simple Storage Services ([S3](https://aws.amazon.com/s3/)). This porcess triggers the S3 bucket event wich starts the training process. A sample of the event payload sent to the SNN framework is as follows:\n",
    "\n",
    ">**Note:** In order for the *2-Layer Sample* to work, please update the following lines in the code below and add the name of the S3 Bucket created during deployment. For example:\n",
    "```json\n",
    "    \"bucket\": {\n",
    "        \"arn\": \"arn:aws:s3:::<BUCKET Name>\",\n",
    "        \"name\": \"<Bucket Name>\",\n",
    "    ...\n",
    "```\n",
    "For this version of the implementation, the S3 Bucket is called **itsacat-demo** and the folder is called **training_input**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simulate S3 event trigger data\n",
    "event = {\n",
    "    \"Records\": [\n",
    "        {\n",
    "            \"eventVersion\": \"2.0\",\n",
    "            \"eventTime\": \"1970-01-01T00:00:00.000Z\",\n",
    "            \"requestParameters\": {\n",
    "                \"sourceIPAddress\": \"127.0.0.1\"\n",
    "             },\n",
    "            \"s3\": {\n",
    "                \"configurationId\": \"testConfigRule\",\n",
    "                \"object\": {\n",
    "                    \"eTag\": \"0123456789abcdef0123456789abcdef\",\n",
    "                    \"sequencer\": \"0A1B2C3D4E5F678901\",\n",
    "                    \"key\": \"training_input/datasets.h5\",\n",
    "                    \"size\": 1024\n",
    "                },\n",
    "                \"bucket\": {\n",
    "                    \"arn\": \"arn:aws:s3:::itsacat-demo\",\n",
    "                    \"name\": \"itsacat-demo\",\n",
    "                    \"ownerIdentity\": {\n",
    "                        \"principalId\": \"EXAMPLE\"\n",
    "                    }\n",
    "                },\n",
    "                \"s3SchemaVersion\": \"1.0\"\n",
    "            },\n",
    "            \"responseElements\": {\n",
    "                \"x-amz-id-2\": \"EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH\",\n",
    "                \"x-amz-request-id\": \"EXAMPLE123456789\"\n",
    "            },\n",
    "            \"awsRegion\": \"us-west-2\",\n",
    "            \"eventName\": \"ObjectCreated:Put\",\n",
    "            \"userIdentity\": {\n",
    "                \"principalId\": \"EXAMPLE\"\n",
    "            },\n",
    "            \"eventSource\": \"aws:s3\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "context = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To establish client connectivity to the various AWS services that the function will leverage, the following code creates the needed clients for the various AWS services, as global variables.\n",
    "\n",
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "s3_client = client('s3', region_name='us-west-2') # S3 low level class object\n",
    "s3_resource = resource('s3') # S3 high level service class\n",
    "lambda_client = client('lambda', region_name='us-west-2') # Lambda invocation client\n",
    "redis_client = client('elasticache', region_name='us-west-2') # ElastiCache low level object\n",
    "\n",
    "# Find and retrieve the Elasticache Cluster endpoint\n",
    "cc = redis_client.describe_cache_clusters(ShowCacheNodeInfo=True)\n",
    "endpoint = cc['CacheClusters'][0]['CacheNodes'][0]['Endpoint']['Address']\n",
    "cache = redis(host=endpoint, port=6379, db=15) # Connect Python to Redis Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Overview\n",
    "### Training and Test Datasets\n",
    "It is **very important** in Neural Network programming (without the use of a Deep Learning Framework), to have a full understanding of the dimensions of the input data as well as how the dimensions are transformed at each layer, therefore to build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat, the following cells explain the datsets.\n",
    "\n",
    "To train the Neural Network, we are provided with a dataset (`datasets.h5`) containing:\n",
    "- a training set of $m$ images containing cats and non-cats as well as the appropriate class labels ($y=1$) and non-cat images ($y=0$).\n",
    "- a test set of $m$ images containing cats and non-cat sas well as the appropriate class labels ($y=1$) and non-cat images ($y=0$).\n",
    "- classes list for cat and non-cat images.\n",
    "\n",
    ">**Note:** The original dataset was comprised of two separate files, `test_catvnoncat.h5` and `train_catvnoncat.h5`. For the sake of this implementation a single file is needed to upload to the *S3 Bucket*, `datasets.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_classes\n",
      "test_set_x\n",
      "test_set_y\n",
      "train_set_x\n",
      "train_set_y\n"
     ]
    }
   ],
   "source": [
    "# Load main dataset that's stored locally\n",
    "dataset = h5py.File('datasets/datasets.h5', \"r\")\n",
    "\n",
    "# Get the names of the unique datsets\n",
    "datasetNames = [n for n in dataset.keys()]\n",
    "for n in datasetNames:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create numpy arrays of the various unique datasets\n",
    "train_set_x_orig = np.array(dataset[\"train_set_x\"][:]) # train set features\n",
    "train_set_y_orig = np.array(dataset[\"train_set_y\"][:]) # train set labels\n",
    "test_set_x_orig = np.array(dataset[\"test_set_x\"][:]) # test set features\n",
    "test_set_y_orig = np.array(dataset[\"test_set_y\"][:]) # test set labels\n",
    "classes = np.array(dataset[\"list_classes\"][:]) # the list of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_orig dimensions: (209, 64, 64, 3)\n",
      "train_set_y_orig dimension: (209,)\n",
      "test_set_x_orig dimensions: (50, 64, 64, 3)\n",
      "test_set_y_orig dimensions: (50,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaye the dimensions of each unique data set\n",
    "print(\"train_set_x_orig dimensions: \" + str(train_set_x_orig.shape))\n",
    "print(\"train_set_y_orig dimension: \" + str(train_set_y_orig.shape))\n",
    "print(\"test_set_x_orig dimensions: \" + str(test_set_x_orig.shape))\n",
    "print(\"test_set_y_orig dimensions: \" + str(test_set_y_orig.shape))\n",
    "test_set_y_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the cell above, the image data (`train_set_x_orig` and `test_set_x_orig`) are 4-dimensional arrays consiting of $209$ training examoples (**m_train**) and $50$ testing images (**m_test**) respectively. Each image is in turn of *height*, *width* and *depth* (**R**ed, **G**reen **B**lue values) of $64 \\times 64 \\times 3$.\n",
    "\n",
    "Additionally, the dimension for the labels (`train_set_y_orig` and `test_set_y_orig`) only show a $209$ and $50$ column structure. So it is recommended when coding new networks, don't use data structures where the shape is $5$, or $n$, rank 1 array. Instead, this is set to, `(1, 209)` and `(1, 50)`, to make them a **row vector**, and in essence add another dimension to the `Numpy` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create row vectors for the labels.\n",
    "train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_y dimensions: (1, 209)\n",
      "test_set_y dimensions: (1, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"train_set_y dimensions: \" + str(train_set_y.shape))\n",
    "print(\"test_set_y dimensions: \" + str(test_set_y.shape))\n",
    "test_set_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Now that the additional dimension has been added to the label data, we can note the additional \"[[ ]]\" when displaying the array.\n",
    "\n",
    "Next we can see the label data and view the corresponding image, in this case, $index = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [1], and therefore it's a 'cat' picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfkTECiJO/IiIFUSc+BERK4j/H6VB4THAcFMnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f709046c1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a cat picture\n",
    "index = 2\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print (\"y = \" + str(train_set_y[:, index]) + \\\n",
    "       \", and therefore it's a '\" + \\\n",
    "       classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") + \\\n",
    "       \"' picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `np.squeeze()` method extracts the \" inner dimension\" of the array, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_y[:, index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(train_set_y[:, index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** The \"[ ]\" has been removed.\n",
    "\n",
    "### Data Preprocessing\n",
    "The final model analysis (see **Analysis** Notebook)is expecting a *training* set and a *test* set represented by a numpy array of shape (no. pixels $\\times$ no. pixels $\\times$ depth, data set size) respectively. In turn, the model is expecting the training set and test set labels represented as a numpy array (vector) of shape (1, data set size) respectively.\n",
    "\n",
    ">**Note:** It is not determined as yet wether the \"vectorization\" of the images should be performed by the `TrainerLambda` to set up the inputs for *Layer 0*. For the sake of Version 1.0, the preprocessing of the input data will be performed by `launch.py` as various helper functions.\n",
    "\n",
    "#### Vectorize\n",
    "The images are represented by a 3D array of shape $(length, height, depth = 3)$. However, when an image is read as the input of an algorithm it is converted to a vector of shape $(length*height*3, 1)$. In other words, it is \"unrolled\", \"flattened\" or \"reshaped\" from a 3D array into a 1D vector as can be seen below.\n",
    "\n",
    "<img src=\"images/vectorization.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><left>[*image source](https://www.deeplearning.ai)</left></caption><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The following cells explains of this process using the `train_set_x_orig` numpy array. The end result for the input to the model is a is a numpy array where where each column represents a flattened image in a matrix with all the input features (images) being a column, $209$ for the training set and $50$ for the test set respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (209, 64, 64, 3)\n",
      "Flattened shape: (209, 12288)\n",
      "Transpose: (12288, 209)\n"
     ]
    }
   ],
   "source": [
    "# Copy of origional training set\n",
    "orig = train_set_x_orig\n",
    "print(\"Original shape: \" + str(orig.shape))\n",
    "\n",
    "# \"vectorize\" or flatten out the array into an 1D vector\n",
    "flatten = orig.reshape(orig.shape[0], -1)\n",
    "print(\"Flattened shape: \"+ str(flatten.shape))\n",
    "\n",
    "# Transpose into a colums\n",
    "flatten_T = flatten.T\n",
    "print(\"Transpose: \" + str(flatten_T.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** For further intuition of what the above code is doing, the following shows a more \"manual\", alternate way.\n",
    "\n",
    "#### Standardize\n",
    "To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from $0$ to $255$. One common preprocessing step in machine learning is to subtract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by $255$ (the maximum value of a pixel channel). \n",
    "\n",
    ">**Note:** During the training of the model, the weights are multiplied and biases added to the initial inputs in order to observe neuron activations. Then it will backward propagate with the gradients to train the model. But, it is extremely important for each feature to have a similar range such that our gradients don't explode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x shape: (12288, 209)\n",
      "sample value: 0.266666666667\n"
     ]
    }
   ],
   "source": [
    "# Load datsets for preprocessing after vectorization\n",
    "train_set_x = (train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T) / 255\n",
    "test_set_x = (test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T) / 255\n",
    "print(\"train_set_x shape: \" + str(train_set_x.shape))\n",
    "print(\"sample value: \" + str(train_set_x[index][index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Function Overview\n",
    "\n",
    "### Helper Functions\n",
    "\n",
    "#### `to_cache(endpoint, obj, name)`  \n",
    "\n",
    "Serializes multiple data type to ElastiCache and returns the Key.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_cache(endpoint, obj, name):\n",
    "    \"\"\"\n",
    "    Serializes multiple data type to ElastiCache and returns\n",
    "    the Key.\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- The ElastiCache endpoint\n",
    "    obj -- the object to srialize. Can be of type:\n",
    "            - Numpy Array\n",
    "            - Python Dictionary\n",
    "            - String\n",
    "            - Integer\n",
    "    name -- Name of the Key\n",
    "    \n",
    "    Returns:\n",
    "    key -- For each type the key is made up of {name}|{type} and for\n",
    "           the case of Numpy Arrays, the Length and Width of the \n",
    "           array are added to the Key.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Test if the object to Serialize is a Numpy Array\n",
    "    if 'numpy' in str(type(obj)):\n",
    "        array_dtype = str(obj.dtype)\n",
    "        if len(obj.shape) == 0:\n",
    "            length = 0\n",
    "            width = 0\n",
    "        else:\n",
    "            length, width = obj.shape\n",
    "        # Convert the array to string\n",
    "        val = obj.ravel().tostring()\n",
    "        # Create a key from the name and necessary parameters from the array\n",
    "        # i.e. {name}|{type}#{length}#{width}\n",
    "        key = '{0}|{1}#{2}#{3}'.format(name, array_dtype, length, width)\n",
    "        # Store the binary string to Redis\n",
    "        cache = redis(host=endpoint, port=6379, db=15)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is a string\n",
    "    elif type(obj) is str:\n",
    "        key = '{0}|{1}'.format(name, 'string')\n",
    "        val = obj\n",
    "        cache = redis(host=endpoint, port=6379, db=15)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is an integer\n",
    "    elif type(obj) is int:\n",
    "        key = '{0}|{1}'.format(name, 'int')\n",
    "        # Convert to a string\n",
    "        val = str(obj)\n",
    "        cache = redis(host=endpoint, port=6379, db=15)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    # Test if the object to serialize is a dictionary\n",
    "    elif type(obj) is dict:\n",
    "        # Convert the dictionary to a String\n",
    "        val = json.dumps(obj)\n",
    "        key = '{0}|{1}'.format(name, 'json')\n",
    "        cache = redis(host=endpoint, port=6379, db=15)\n",
    "        cache.set(key, val)\n",
    "        return key\n",
    "    else:\n",
    "        print(str(type(obj)) + \"is not a supported serialization type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `from_cache(endpoint, key)`\n",
    "De-serializes binary object from ElastiCache by reading the type of object from the name and converting it to the appropriate data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def from_cache(endpoint, key):\n",
    "    \"\"\"\n",
    "    De-serializes binary object from ElastiCache by reading\n",
    "    the type of object from the name and converting it to\n",
    "    the appropriate data type\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- ElastiCache endpoint\n",
    "    key -- Name of the Key to retrieve the object\n",
    "    \n",
    "    Returns:\n",
    "    obj -- The object converted to specifed data type\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the Key is for a Numpy array containing\n",
    "    # `float64` data types\n",
    "    if 'float64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=15)\n",
    "        val = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        if int(length) == 0:\n",
    "            obj = np.float64(np.fromstring(val))\n",
    "        else:\n",
    "            obj = np.fromstring(val, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return obj\n",
    "    # Check if the Key is for a Numpy array containing\n",
    "    # `int64` data types\n",
    "    elif 'int64' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=15)\n",
    "        val = cache.get(key)\n",
    "        # De-serialize the value\n",
    "        array_dtype, length, width = key.split('|')[1].split('#')\n",
    "        obj = np.fromstring(val, dtype=array_dtype).reshape(int(length), int(width))\n",
    "        return obj\n",
    "    # Check if the Key is for a json type\n",
    "    elif 'json' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=15)\n",
    "        obj = cache.get(key)\n",
    "        return json.loads(obj)\n",
    "    # Chec if the Key is an integer\n",
    "    elif 'int' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=15)\n",
    "        obj = cache.get(key)\n",
    "        return int(obj)\n",
    "    # Check if the Key is a string\n",
    "    elif 'string' in key:\n",
    "        cache = redis(host=endpoint, port=6379, db=15)\n",
    "        obj = cache.get(key)\n",
    "        return obj\n",
    "    else:\n",
    "        print(str(type(obj)) + \"is not a supported serialization type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `name2str(obj, namespace)`\n",
    "Converts the name of the numpy array to string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name2str(obj, namespace):\n",
    "    \"\"\"\n",
    "    Converts the name of the numpy array to string\n",
    "    \n",
    "    Arguments:\n",
    "    obj -- Numpy array object\n",
    "    namespace -- dictionary of the current global symbol table\n",
    "    \n",
    "    Return:\n",
    "    List of the names of the Numpy arrays\n",
    "    \"\"\"\n",
    "    return [name for name in namespace if namespace[name] is obj]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Side Note**: An alternate method to *List Comprehension* is to use the `chain()` function to get the names of the Numpy arrays.  \n",
    "```python\n",
    "    from itertools import chain\n",
    "    list(chain.from_iterable(a_names))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `vectorize()`\n",
    "Reshapes (flattens) the image data to column vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(x_orig):\n",
    "    \"\"\"\n",
    "    Vectorize the image data into a matrix of column vectors\n",
    "    \n",
    "    Argument:\n",
    "    x_orig -- Numpy array of image data\n",
    "    \n",
    "    Return:\n",
    "    Reshaped/Transposed Numpy array\n",
    "    \"\"\"\n",
    "    return x_orig.reshape(x_orig.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `standardize()`\n",
    "Preprocess the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(x_orig):\n",
    "    \"\"\"\n",
    "    Standardize the input data\n",
    "    \n",
    "    Argument:\n",
    "    x_orig -- Numpy array of image data\n",
    "    \n",
    "    Return:\n",
    "    Call to `vectorize()`, standardized Numpy array of image data\n",
    "    \"\"\"\n",
    "    return vectorize(x_orig) / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `initialize_data(endpoint, parameters)`\n",
    "Extracts the training and testing data from S3, flattens, standardizes and then dumps the data to ElastiCache for neurons to process as layer $a^0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_data(endpoint, parameters):\n",
    "    \"\"\"\n",
    "    Extracts the training and testing data from S3, flattens, \n",
    "    standardizes and then dumps the data to ElastiCache \n",
    "    for neurons to process as layer a^0.\n",
    "    \n",
    "    Arguments:\n",
    "    endpoint -- The ElastiCache endpoint\n",
    "    parameters -- The initial/running parameters dictionary object\n",
    "    \n",
    "    Returns:\n",
    "    data_keys -- Hash keys for the various numpy arrays\n",
    "    input_data -- Reference for the Input data extracted for the h5 file\n",
    "    dims -- Reference to the dimensions of the input data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load main dataset\n",
    "    dataset = h5py.File('/tmp/datasets.h5', \"r\")\n",
    "    \n",
    "    # Create numpy arrays from the various h5 datasets\n",
    "    train_set_x_orig = np.array(dataset[\"train_set_x\"][:]) # train set features\n",
    "    train_set_y_orig = np.array(dataset[\"train_set_y\"][:]) # train set labels\n",
    "    test_set_x_orig = np.array(dataset[\"test_set_x\"][:]) # test set features\n",
    "    test_set_y_orig = np.array(dataset[\"test_set_y\"][:]) # test set labels\n",
    "    \n",
    "    # Reshape labels\n",
    "    train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "    # Preprocess inputs\n",
    "    train_set_x = standardize(train_set_x_orig)\n",
    "    test_set_x = standardize(test_set_x_orig)\n",
    "    \n",
    "    # Create necessary keys for the data in ElastiCache\n",
    "    data_keys = {} # Dictionary for the hash keys of the data set\n",
    "    dims = {} # Dictionary of data set dimensions\n",
    "    a_list = [train_set_x, train_set_y, test_set_x, test_set_y]\n",
    "    a_names = [] # Placeholder for array names\n",
    "    for i in range(len(a_list)):\n",
    "        # Create a lis of the names of the numpy arrays\n",
    "        a_names.append(name2str(a_list[i], locals()))\n",
    "    for j in range(len(a_list)):\n",
    "        # Dump the numpy arrays to ElastiCache\n",
    "        data_keys[str(a_names[j][0])] = to_cache(endpoint=endpoint, obj=a_list[j], name=a_names[j][0])\n",
    "        # Append the array dimensions to the list\n",
    "        dims[str(a_names[j][0])] = a_list[j].shape\n",
    "    \n",
    "    # Initialize A0 and Y names from `train_setx` and `train_set_y`\n",
    "    data_keys['A0'] = to_cache(endpoint=endpoint, obj=train_set_x, name='A0')\n",
    "    data_keys['Y'] = to_cache(endpoint=endpoint, obj=train_set_y, name='Y')\n",
    "    # Initialize training example size\n",
    "    m = train_set_x.shape[1]\n",
    "    data_keys['m'] = to_cache(endpoint, obj=m, name='m')\n",
    "    \n",
    "    # Multiple layer weight and bias initialization\n",
    "    for l in range(1, parameters['layers']+1):\n",
    "        if l == 1:\n",
    "            W = np.random.randn(parameters['neurons']['layer'+str(l)], train_set_x.shape[0]) / np.sqrt(train_set_x.shape[0])\n",
    "        else:\n",
    "            W = np.random.randn(parameters['neurons']['layer'+str(l)], parameters['neurons']['layer'+str(l-1)]) / np.sqrt(parameters['neurons']['layer'+str(l-1)])\n",
    "        b = np.zeros((parameters['neurons']['layer'+str(l)], 1))\n",
    "        # Store the initial weights and bias in ElastiCache\n",
    "        data_keys['W'+str(l)] = to_cache(endpoint=endpoint, obj=W, name='W'+str(l))\n",
    "        data_keys['b'+str(l)] = to_cache(endpoint=endpoint, obj=b, name='b'+str(l))\n",
    "    \n",
    "    # Initialize the results tracking object\n",
    "    results = {}\n",
    "    data_keys['results'] = to_cache(endpoint, obj=results, name='results')\n",
    "\n",
    "    return data_keys, [j for i in a_names for j in i], dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `numpy2s3(array, name, bucket)`\n",
    "Serialize a Numpy array to S3 without using local copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numpy2s3(array, name, bucket):\n",
    "    \"\"\"\n",
    "    Serialize a Numpy array to S3 without using local copy\n",
    "    \n",
    "    Arguments:\n",
    "    array -- Numpy array of any shape\n",
    "    name -- filename on S3\n",
    "    bucket -- S3 Bucket name\n",
    "    \"\"\"\n",
    "    f_out = io.BytesIO()\n",
    "    np.save(f_out, array)\n",
    "    try:\n",
    "        s3_client.put_object(Key=name, Bucket=bucket, Body=f_out.getvalue(), ACL='bucket-owner-full-control')\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `start_epoch(epoch, layer, parameter_key)`\n",
    "Starts a new epoch and configures the necessary state tracking objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def start_epoch(epoch, layer, parameter_key):\n",
    "    \"\"\"\n",
    "    Starts a new epoch and configures the necessary state tracking objcts.\n",
    "    \n",
    "    Arguments:\n",
    "    epoch -- Integer representing the \"current\" epoch.\n",
    "    layer -- Integer representing the current hidden layer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the results object for the new epoch\n",
    "    parameters = from_cache(endpoint=endpoint, key=parameter_key)\n",
    "    \n",
    "    # Add current epoch to results\n",
    "    epoch2results = from_cache(endpoint=endpoint, key=parameters['data_keys']['results'])\n",
    "    epoch2results['epoch' + str(epoch)] = {}\n",
    "    parameters['data_keys']['results'] = to_cache(endpoint=endpoint, obj=epoch2results, name='results')\n",
    "   \n",
    "    # Update parameters with this functions data\n",
    "    parameters['epoch'] = epoch\n",
    "    parameters['layer'] = layer\n",
    "    parameter_key = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "    \n",
    "    # Start forwardprop\n",
    "    propogate(direction='forward', epoch=epoch, layer=layer+1, parameter_key=parameter_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `end(parameter_key)`\n",
    "Finishes the oveall training sequence and saves the \"optmized\" weights and bias to S3, for the prediction aplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def end(parameter_key):\n",
    "    \"\"\"\n",
    "    Finishes the overall training sequence and saves the \"optimized\" \n",
    "    weights and bias to S3, for the prediction application.\n",
    "    \n",
    "    Arguments:\n",
    "    parameter_key -- The ElastiCache key for the current set of state parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the latest parameters\n",
    "    parameters = from_cache(\n",
    "        endpoint=endpoint,\n",
    "        key=parameter_key\n",
    "    )\n",
    "\n",
    "    # Get the results key\n",
    "    final_results = from_cache(\n",
    "        endpoint=endpoint,\n",
    "        key=parameters['data_keys']['results']\n",
    "    )\n",
    "    # Upload the final results to S3\n",
    "    bucket = parameters['s3_bucket']\n",
    "    results_obj = s3_resource.Object(bucket,'training_results/results.json')\n",
    "    try:\n",
    "        results_obj.put(Body=json.dumps(final_results))\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        print(e)\n",
    "        raise\n",
    "    \n",
    "    # Get the final weights and bias for each layer and upload them to S3 for\n",
    "    # use by the prediction and analysis\n",
    "    # Create dictionary of model parameters for prediction app\n",
    "    params = {}\n",
    "    for l in range(1, parameters['layers']+1):\n",
    "        params['W'+str(l)] = from_cache(endpoint=endpoint, key=parameters['data_keys']['W'+str(l)])\n",
    "        params['b'+str(l)] = from_cache(endpoint=endpoint, key=parameters['data_keys']['b'+str(l)])\n",
    "    # Create a model parameters file for use by prediction app\n",
    "    with h5py.File('/tmp/params.h5', 'w') as h5file:\n",
    "        for key in params:\n",
    "            h5file['/' + key] = params[key]\n",
    "    # Upload model parameters file to S3\n",
    "    s3_resource.Object(bucket, 'predict_input/params.h5').put(Body=open('/tmp/params.h5', 'rb'))\n",
    "\n",
    "    print(\"Training Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `propogate(direction, epoch, layer, parameter_key)`\n",
    "Determines the amount of \"hidden\" units based on the layer and loops through launching the necessary `NeuronLambda` functions with the appropriate state or \"direction\" to propagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def propogate(direction, epoch, layer, parameter_key):\n",
    "    \"\"\"\n",
    "    Determines the amount of \"hidden\" units based on the layer and loops\n",
    "    through launching the necessary `NeuronLambda` functions with the \n",
    "    appropriate state. Each `NeuronLambda` implements the cost function \n",
    "    OR the gradients depending on the direction.\n",
    "\n",
    "    Arguments:\n",
    "    direction -- The current direction of the propogation, either `forward` or `backward`.\n",
    "    epoch -- Integer representing the \"current\" epoch to close out.\n",
    "    layer -- Integer representing the current hidden layer.\n",
    "\n",
    "    Note: When launching NeuronLambda with multiple hidden unit,\n",
    "    remember to assign an ID, also remember to start at 1\n",
    "    and not 0. for example:\n",
    "    num_hidden_units = 5\n",
    "    for i in range(1, num_hidden_units + 1):\n",
    "        # Do stuff\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the parameters for the layer\n",
    "    parameters = from_cache(endpoint=endpoint, key=parameter_key)\n",
    "    num_hidden_units = parameters['neurons']['layer' + str(layer)]\n",
    "    \n",
    "    # Build the NeuronLambda payload\n",
    "    payload = {}\n",
    "    # Add the parameters to the payload\n",
    "    payload['state'] = direction\n",
    "    payload['parameter_key'] = parameter_key\n",
    "    payload['epoch'] = epoch\n",
    "    payload['layer'] = layer\n",
    "\n",
    "    # Determine process based on direction\n",
    "    if direction == 'forward':\n",
    "        # Launch Lambdas to propagate forward\n",
    "        # Prepare the payload for `NeuronLambda`\n",
    "        # Update parameters with this function's updates\n",
    "        parameters['epoch'] = epoch\n",
    "        parameters['layer'] = layer\n",
    "        payload['parameter_key'] = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "\n",
    "        # Debug Statements\n",
    "        #print(\"Starting Forward Propagation for epoch \" + str(epoch) + \", layer \" + str(layer))\n",
    "\n",
    "        for i in range(1, num_hidden_units + 1):\n",
    "            # Prepare the payload for `NeuronLambda`\n",
    "            payload['id'] = i\n",
    "            if i == num_hidden_units:\n",
    "                payload['last'] = \"True\"\n",
    "            else:\n",
    "                payload['last'] = \"False\"\n",
    "            payload['activation'] = parameters['activations']['layer' + str(layer)]\n",
    "            #payloadbytes = dumps(payload)\n",
    "            # Debug Statements\n",
    "            #print(\"Payload to be sent NeuronLambda: \\n\" + dumps(payload, indent=4, sort_keys=True))\n",
    "            neuron_handler(event=payload, context='')\n",
    "        \n",
    "        return\n",
    "    \n",
    "    elif direction == 'backward':\n",
    "        # Launch Lambdas to propagate backward        \n",
    "        # Prepare the payload for `NeuronLambda`\n",
    "        # Update parameters with this functions updates\n",
    "        parameters['epoch'] = epoch\n",
    "        parameters['layer'] = layer\n",
    "        payload['parameter_key'] = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "\n",
    "        # Debug Statement\n",
    "        #print(\"Starting Backward Propogation for epoch \" + str(epoch) + \", layer \" + str(layer))\n",
    "\n",
    "        for i in range(1, num_hidden_units + 1):\n",
    "            # Prepare the payload for `NeuronLambda`\n",
    "            payload['id'] = i\n",
    "            if i == num_hidden_units:\n",
    "                payload['last'] = \"True\"\n",
    "            else:\n",
    "                payload['last'] = \"False\"\n",
    "            payload['activation'] = parameters['activations']['layer' + str(layer)]\n",
    "            payloadbytes = dumps(payload)\n",
    "            # Debug statements\n",
    "            #print(\"Payload to be sent to NeuronLambda: \\n\" + dumps(payload, indent=4, sort_keys=True))\n",
    "            \n",
    "            neuron_handler(event=payload, context='')\n",
    "        \n",
    "        return\n",
    "    \n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sigmoid(z)`\n",
    "Computes the sigmoid of `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size\n",
    "\n",
    "    Return:\n",
    "    sigmoid(z)\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sigmoid_backward(dA, z)`\n",
    "Computes the derivative of the sigmoid function, given `z`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, z):\n",
    "    \"\"\"\n",
    "    Implement the derivative of the sigmoid function\n",
    "\n",
    "    Arguments:\n",
    "    dA -- Post-activation gradient, of any shape\n",
    "    z -- Cached linear activation from Forward prop\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the Cost with respect to z\n",
    "    \"\"\"\n",
    "    s = 1. / (1. + np.exp(-z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "    # Debug statement\n",
    "    assert(dZ.shape == z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `relu(z)`\n",
    "Implements the Rectified Linear Unit function of `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \"\"\"\n",
    "    Implement the ReLU function.\n",
    "\n",
    "    Arguments:\n",
    "    z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    a -- Post-activation parameter, of the same shape as z\n",
    "    \"\"\"\n",
    "    a = np.maximum(0, z)\n",
    "    assert(a.shape == z.shape)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `relu_backward(dA, z)`\n",
    "Implement the backward propagation for a single ReLU unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dA, z):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single ReLU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- Post-activation gradient, of any shape\n",
    "    z -- Cached linear activation from Forward propagation\n",
    "\n",
    "    Returns:\n",
    "    dz -- Gradient of the cost with respect to z\n",
    "    \"\"\"\n",
    "    dz = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dz[z <= 0] = 0\n",
    "    assert (dz.shape == z.shape)\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `vectorizer(Outputs, Layer)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizer(Outputs, Layer):\n",
    "    \"\"\"\n",
    "    Creates a matrix of the individual neuron output for better vectorization\n",
    "    \n",
    "    Arguments:\n",
    "    Outputs -- ElastiCache key to search for the data from `NeuronLambda`\n",
    "               e.g. 'a' for activations; 'dw' for Weight Derivatives\n",
    "    Layer -- Layer to search for neuron output that need to vectorized\n",
    "    \n",
    "    Returns:\n",
    "    result -- Matrix matching the size for the entire layer\n",
    "    \"\"\"\n",
    "    # Use the following Redis command to ensure a pure string is return for the key\n",
    "    r = redis(host=endpoint, port=6379, db=15, charset=\"utf-8\", decode_responses=True)\n",
    "    search_results = []\n",
    "    # Compile a list of all the neurons in the search layer based on the search criteria\n",
    "    for n in range(1, parameters['neurons']['layer'+str(Layer)]+1):\n",
    "        tmp = r.keys('layer'+str(Layer)+'_'+str(Outputs)+'_'+str(n)+'|*')\n",
    "        search_results.append(tmp)\n",
    "    # Created an ordered list of neuron data keys\n",
    "    key_list = []\n",
    "    for result in search_results:\n",
    "        key_list.append(result[0])\n",
    "    # Create a dictionary of neuron data\n",
    "    Dict = {}\n",
    "    for data in key_list:\n",
    "        Dict[data] = from_cache(endpoint=endpoint, key=data)\n",
    "    # Number of Neuron Activations for the search layer\n",
    "    num_neurons = parameters['neurons']['layer'+str(Layer)]\n",
    "    # Create a numpy array of the results, depending on the number\n",
    "    # of neurons (a Matrix of Activations)\n",
    "    result = np.array([arr.tolist() for arr in Dict.values()])\n",
    "    if num_neurons == 1:\n",
    "        # Single Neuron Activation\n",
    "        dims = (key_list[0].split('|')[1].split('#')[1:])\n",
    "        result = result.reshape(int(dims[0]), int(dims[1]))\n",
    "    else:\n",
    "        # Multiple Neuron Activations\n",
    "        result = np.squeeze(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda Handler Functions\n",
    "#### `launch_handler(event, context)`\n",
    "This `lambda_handler()` is triggered by the S3 event where training data is uploaded to S3. It further initializes the various components needed, such as:\n",
    "1. State tracking Objects:\n",
    "    - Overall Results (Cost) for each Epoch.\n",
    "    - Gradients for each layer.\n",
    "    - Initial and updated Weight parameter for each layer.\n",
    "    - Initial and updated Bias parameter for each layer.\n",
    "2. DynamoDB Storage:\n",
    "    - Invocation ID for each Lambda Function invocation to prevent duplicate invocation.\n",
    "    >**Note:** The DynamoDB Initialization is **NOT** recorded within the **Codebook**.\n",
    "3. Preprocessing the Input Data: \n",
    "    - Read in the the initial *training*, *test* of Cat and Non-cat images.\n",
    "    - The function initially loads the data in `h5py` format and extracts the *training* and *test* data.\n",
    "    - The function further performs any standardization and normalization of the input data.\n",
    "    - The function also \"*flattens*\" the data into a column vector, thus performing **Vectorization**.\n",
    "    - This data is dumped to ElastiCache and will thus serve as **Layer 0** of the Neural Network.\n",
    "4. Environment and State Tracking Variables:\n",
    "    - Loading the initial Neural Network Parameters (`parameters.json`) and augmenting these parameter with the state variables during the training process. The settings include overall parameters used by the `trainer` and `neuron` Lambda Functions, such as:\n",
    "        - Total number of epochs/iterations.\n",
    "        - Total number of layers in the Neural Network (including the Output layer).\n",
    "        - Total number of \"neurons\" in each layer.\n",
    "        - The activation function to be used for each layer.\n",
    "    - Initializing the **Hash Keys** for the various data sets in ElastiCache to be used by the subsequent functions to get access to the numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def launch_handler(event, context):\n",
    "    # Retrieve datasets and setting from S3\n",
    "    input_bucket = s3_resource.Bucket(str(event['Records'][0]['s3']['bucket']['name']))\n",
    "    dataset_key = str(event['Records'][0]['s3']['object']['key'])\n",
    "    settings_key = dataset_key.split('/')[-2] + '/parameters.json'\n",
    "    try:\n",
    "        input_bucket.download_file(dataset_key, '/tmp/datasets.h5')\n",
    "        #input_bucket.download_file(settings_key, '/tmp/parameters.json')\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == '404':\n",
    "            print(\"Error downloading input data from S3, S3 object does not exist\")\n",
    "        else:\n",
    "            raise\n",
    "    \n",
    "    # Extract the neural network parameters\n",
    "    with open('datasets/sample_parameters.json') as parameters_file:\n",
    "        parameters = json.load(parameters_file)\n",
    "    \n",
    "    # Build in additional neural network parameters\n",
    "    # Input data sets and data set parameters\n",
    "    parameters['s3_bucket'] = event['Records'][0]['s3']['bucket']['name']\n",
    "    parameters['data_keys'],\\\n",
    "    parameters['input_data'],\\\n",
    "    parameters['dims'] = initialize_data(\n",
    "        endpoint=endpoint,\n",
    "        parameters=parameters\n",
    "    )\n",
    "    \n",
    "    # Initialize payload to `TrainerLambda`\n",
    "    payload = {}\n",
    "    # Initialize the overall state\n",
    "    payload['state'] = 'start'\n",
    "    # Dump the parameters to ElastiCache\n",
    "    payload['parameter_key'] = to_cache(endpoint, obj=parameters, name='parameters')\n",
    "    #payload['endpoint'] = endpoint\n",
    "    # Prepare the payload for `TrainerLambda`\n",
    "    #payloadbytes = dumps(payload)\n",
    "    \n",
    "    # Debug statements\n",
    "    print(\"Complete Neural Network Settings: \\n\")\n",
    "    print(dumps(parameters, indent=4, sort_keys=True))\n",
    "    print(\"Payload to be sent to TrainerLambda: \\n\" + dumps(payload, indent=4, sort_keys=True))\n",
    "    trainer_handler(event=payload, context='')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `trainer_handler(event, context)`\n",
    "This `lambda_handler()` function is the most critical function in the set in that it:\n",
    "1. Tracks and updates the state across the interactions/epochs and the various layers of the Neural Network.\n",
    "2. Performs Vectorization on the Activation Row Vectors from each Neuron to create a *Matrix* of Activations.\n",
    "3. Launches the various Neurons (`NeuronLamabda`) in each layer and tracks their output for *Forward* or *Backward* propogation.\n",
    "4. Calculates the *Cost* for each iteration of *Forward* propagation.\n",
    "5. Performs *Gradient Descent* for each Epoch.\n",
    "\n",
    "In order to accomplish this, the `TrainerLambda` has three possible states, `start`, `forward` and `backward`:\n",
    "1. `start`: This state starts the initial or subsequent training epochs and performs the following:\n",
    "    - Initializes the new weights and bias for the epoch.\n",
    "    - Updates the state table with these values.\n",
    "2. `forward`: This state processes the *forward* propagation step and launches the various hidden layer Neurons and supplies the necessary state information to these functions, such as:\n",
    "    - Input/Activation data location\n",
    "    - Weights and Bias.\n",
    "    - Hidden Layer No.\n",
    "    - Number of Hidden Units.\n",
    "    - Activation Funciton for the Layer.\n",
    "3. `backward`: This state processes the *back* propagation step and launches the various hidden layer Neurons as well as supplies the necessary information for these functions, like:\n",
    "    - Hidden Layer No.\n",
    "    - Number of Hidden Units.\n",
    "    - Current and previous Activations calculated from the forward propagation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainer_handler(event, context):\n",
    "    \"\"\"\n",
    "    1. Processes the `event` variables from the various Lambda functions that call it, \n",
    "        i.e. `TrainerLambda` and `NeuronLambda`.\n",
    "    2. Determines the \"current\" state and then directs the next steps.\n",
    "    3. Performs Vectorization from the NeuronLambda forward propagation outputs.\n",
    "    4. Calculates the Cost.\n",
    "    5. Performs Gradient Descent given the gradients from the backward propagation outputs.\n",
    "    \"\"\"\n",
    "    # Get the current state from the invoking lambda\n",
    "    state = event.get('state')\n",
    "    global parameters\n",
    "    parameters = from_cache(endpoint=endpoint, key=event.get('parameter_key'))\n",
    "    \n",
    "    # Execute appropriate action based on the the current state\n",
    "    if state == 'forward':\n",
    "        # Perform vectorization to create a matrix of activations and/or calculate the Cost\n",
    "        # Get important state variables\n",
    "        epoch = event.get('epoch')\n",
    "        layer = event.get('layer')\n",
    "\n",
    "        # Get the Vectorized matrix of Activations\n",
    "        A = vectorizer(Outputs='a', Layer=layer-1)\n",
    "\n",
    "        # Add the `A` Matrix to `data_keys` for later Neuron use\n",
    "        A_name = 'A' + str(layer-1)\n",
    "        parameters['data_keys'][A_name] = to_cache(endpoint=endpoint, obj=A, name=A_name)\n",
    "\n",
    "        # Update ElastiCache with this function's data\n",
    "        parameter_key = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "        \n",
    "        # Determine the location within forwardprop\n",
    "        if layer > parameters['layers']:\n",
    "            # Location is at the end of forwardprop, therefore calculate Cost\n",
    "            # Get the training examples data and no. examples (`Y` and `m`)\n",
    "            Y = from_cache(endpoint=endpoint, key=parameters['data_keys']['Y'])\n",
    "            m = from_cache(endpoint=endpoint, key=parameters['data_keys']['m'])\n",
    "            \n",
    "            # Calculate the Cost\n",
    "            #cost = -1 / m * np.sum(np.multiply(Y, np.log(A)) + np.multiply((1 - Y), np.log(1 - A))) #from ste-by-step\n",
    "            cost = (1./m) * (-np.dot(Y, np.log(A).T) - np.dot(1 - Y, np.log(1 - A).T)) #from application\n",
    "            \"\"\"\n",
    "            Note: The cost calculation above returns `nd.array`, therefore converting to\n",
    "            type `float` for for the results upload\n",
    "            \"\"\"\n",
    "            #cost = (-1 / m) * np.sum(Y * (np.log(A)) + ((1 - Y) * np.log(1 - A))) #from S-Layer\n",
    "            cost = np.squeeze(cost)\n",
    "            assert(cost.shape == ())\n",
    "\n",
    "            # Update results with the Cost\n",
    "            # Get the results object\n",
    "            cost2results = from_cache(endpoint=endpoint, key=parameters['data_keys']['results'])\n",
    "            # Append the cost to results object\n",
    "            #cost2results['epoch' + str(epoch)]['cost'] = cost\n",
    "            cost2results['epoch' + str(epoch)]['cost'] = float(cost)\n",
    "            # Update results key in ElastiCache\n",
    "            parameters['data_keys']['results'] = to_cache(endpoint=endpoint, obj=cost2results, name='results')\n",
    "\n",
    "            #if epoch % 100 == 0:\n",
    "                #print(\"Cost after epoch {0}: {1}\".format(epoch, cost))\n",
    "            print(\"Cost after epoch {}: {}\".format(epoch, cost))\n",
    "\n",
    "            # Initialize backprop\n",
    "            # Calculate the derivative of the Cost with respect to the last activation\n",
    "            # Ensure that `Y` is the correct shape as the last activation\n",
    "            Y = Y.reshape(A.shape)\n",
    "            dA = - (np.divide(Y, A) - np.divide(1 - Y, 1 - A))\n",
    "            dA_name = 'dA' + str(layer-1)\n",
    "            parameters['data_keys'][dA_name] = to_cache(endpoint=endpoint, obj=dA, name=dA_name)\n",
    "\n",
    "            # Update parameters from this function in ElastiCache\n",
    "            parameter_key = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "\n",
    "            # Start Backpropagation on NeuronLambda\n",
    "            propogate(direction='backward', epoch=epoch, layer=layer-1, parameter_key=parameter_key)\n",
    "\n",
    "        else:\n",
    "            # Move to the next hidden layer for multiple layer networks\n",
    "            #debug\n",
    "            #print(\"Propagating forward onto Layer \" + str(layer))\n",
    "            propogate(direction='forward', epoch=epoch, layer=layer, parameter_key=parameter_key)\n",
    "        \n",
    "    elif state == 'backward':\n",
    "        # Get important state variables\n",
    "        epoch = event.get('epoch')\n",
    "        layer = event.get('layer')\n",
    "\n",
    "        # Vectorize the derivatives\n",
    "        dZ = vectorizer(Outputs='dZ', Layer=layer+1)\n",
    "        \n",
    "        # Next pre-process the derivative of the weights\n",
    "        dW = vectorizer(Outputs='dw', Layer=layer+1)\n",
    "        \n",
    "        # pre-process the derivatives of the bias\n",
    "        db = vectorizer(Outputs='db', Layer=layer+1)\n",
    "        db = db.reshape(db.shape[0], 1)\n",
    "        \n",
    "        # Determine the location within backprop\n",
    "        if epoch == parameters['epochs']-1 and layer == 0:\n",
    "            # Location is at the end of the final epoch\n",
    "            # Get relavent parameters for bacprop\n",
    "            W = from_cache(endpoint=endpoint, key=parameters['data_keys']['W'+str(layer+1)])\n",
    "            b = from_cache(endpoint=endpoint, key=parameters['data_keys']['b'+str(layer+1)])\n",
    "            learning_rate = parameters['learning_rate']\n",
    "            # Run Gradient Descent\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "\n",
    "            # Update ElastiCache with the new Weights and new Bias to be used as the inputs for\n",
    "            # the next epoch\n",
    "            parameters['data_keys']['W'+str(layer+1)] = to_cache(\n",
    "                endpoint=endpoint,\n",
    "                obj=W,\n",
    "                name='W'+str(layer+1)\n",
    "            )\n",
    "            parameters['data_keys']['b'+str(layer+1)] = to_cache(\n",
    "                endpoint=endpoint,\n",
    "                obj=b,\n",
    "                name='b'+str(layer+1)\n",
    "            )\n",
    "            \n",
    "            # Update parameters for the next epoch\n",
    "            parameter_key = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "                        \n",
    "            # Finalize the the process and clean up\n",
    "            end(parameter_key=parameter_key)\n",
    "            \n",
    "        elif epoch < parameters['epochs']-1 and layer == 0:\n",
    "            # Location is at the end of the current epoch and backprop is finished\n",
    "            # Get relevent parameters for backprop\n",
    "            W = from_cache(endpoint=endpoint, key=parameters['data_keys']['W'+str(layer+1)])\n",
    "            b = from_cache(endpoint=endpoint, key=parameters['data_keys']['b'+str(layer+1)])\n",
    "            learning_rate = parameters['learning_rate']\n",
    "            # Run Gradient Descent\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "\n",
    "            # Update ElastiCache with the new Weights and new Bias to be used as the inputs for\n",
    "            # the next epoch\n",
    "            parameters['data_keys']['W'+str(layer+1)] = to_cache(\n",
    "                endpoint=endpoint,\n",
    "                obj=W,\n",
    "                name='W'+str(layer+1)\n",
    "            )\n",
    "            parameters['data_keys']['b'+str(layer+1)] = to_cache(\n",
    "                endpoint=endpoint,\n",
    "                obj=b,\n",
    "                name='b'+str(layer+1)\n",
    "            )\n",
    "            \n",
    "            # Update parameters for the next epoch\n",
    "            parameter_key = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "                        \n",
    "            # Start the next epoch\n",
    "            start_epoch(epoch=epoch+1, layer=0, parameter_key=parameter_key)\n",
    "            \n",
    "        else:\n",
    "            # Location is still within the backprop process, therefore calculate \n",
    "            # the derivative of the current layer's activations with respect to the \n",
    "            # Cost as well as perform gradient decent to get and new weights and bias\n",
    "            # Get relevent parameters for backprop\n",
    "            W = from_cache(endpoint=endpoint, key=parameters['data_keys']['W'+str(layer+1)])\n",
    "            b = from_cache(endpoint=endpoint, key=parameters['data_keys']['b'+str(layer+1)])\n",
    "            learning_rate = parameters['learning_rate']\n",
    "            dA = np.dot(W.T, dZ)\n",
    "            dA_name = 'dA' + str(layer)\n",
    "            parameters['data_keys'][dA_name] = to_cache(endpoint=endpoint, obj=dA, name=dA_name)\n",
    "\n",
    "            # Run Gradient Descent\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "            # Update ElastiCache with the new Weights and new Bias to be used as the inputs for\n",
    "            # the next epoch\n",
    "            parameters['data_keys']['W'+str(layer+1)] = to_cache(\n",
    "                endpoint=endpoint,\n",
    "                obj=W,\n",
    "                name='W'+str(layer+1)\n",
    "            )\n",
    "            parameters['data_keys']['b'+str(layer+1)] = to_cache(\n",
    "                endpoint=endpoint,\n",
    "                obj=b,\n",
    "                name='b'+str(layer+1)\n",
    "            )\n",
    "\n",
    "            # Update parameters from this function in ElastiCache\n",
    "            parameter_key = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "\n",
    "            # Move to the next hidden layer\n",
    "            propogate(direction='backward', epoch=epoch, layer=layer, parameter_key=parameter_key)\n",
    "            \n",
    "    elif state == 'start':\n",
    "        # Start training process        \n",
    "        # Create initial parameters\n",
    "        epoch = 0\n",
    "        layer = 0\n",
    "        start_epoch(epoch=epoch, layer=layer, parameter_key=event.get('parameter_key'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `neuron_handler(event, context)`\n",
    "This `lambda_handler()` simulates a single *Perceptron* for both forward and backward propagation. If the state is `forward` then the function simulates forward propagation for $X$ to $Cost$ for the current layer. If the state is backward, then the function calculates the gradient of the derivative of the activation function for the current layer.\n",
    "\n",
    ">**Note:** This function also moves the state to the next or previous layer, depending on the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neuron_handler(event, context):\n",
    "    \"\"\"\n",
    "    This Lambda Funciton simulates a single Perceptron for both \n",
    "    forward and backward propogation.\n",
    "    \"\"\"    \n",
    "    # Get the Neural Network parameters from Elasticache\n",
    "    parameters = from_cache(endpoint, key=event.get('parameter_key'))\n",
    "       \n",
    "    # Get the current state\n",
    "    state = event.get('state')\n",
    "    epoch = event.get('epoch')\n",
    "    layer = event.get('layer')\n",
    "    ID = event.get('id') # To be used when multiple activations\n",
    "    # Determine is this is the last Neuron in the layer\n",
    "    last = event.get('last')\n",
    "    activation = event.get('activation')\n",
    "    # Debug Statement\n",
    "    #print(\"Starting {} propagation on Neuron: {}, for Epoch {} and Layer {}\".format(state, str(ID), str(epoch), str(layer)))\n",
    "\n",
    "    if state == 'forward':\n",
    "        # Forward propagation from A0 to Cost\n",
    "        # Activations from the previous layer\n",
    "        A_prev = from_cache(endpoint=endpoint, key=parameters['data_keys']['A'+str(layer - 1)])\n",
    "        # Get the weights for this neuron\n",
    "        w = from_cache(endpoint=endpoint, key=parameters['data_keys']['W'+str(layer)])[ID-1, :]\n",
    "        # Convert weights to a row vector\n",
    "        w = w.reshape(1, w.shape[0])\n",
    "        # Get the bias for this neuron as row vector\n",
    "        b = from_cache(endpoint=endpoint, key=parameters['data_keys']['b'+str(layer)])[ID-1, :]\n",
    "        # Perform the linear part of the layer's forward propagation\n",
    "        z = np.dot(w, A_prev) + b\n",
    "        # Upload the linear transformation results to ElastiCache for use with Backprop\n",
    "        to_cache(endpoint=endpoint, obj=z, name='layer'+str(layer)+'_z_'+str(ID))\n",
    "\n",
    "        # Perform non-linear activation based on the activation function\n",
    "        if activation == 'sigmoid':\n",
    "            a = sigmoid(z)\n",
    "        elif activation == 'relu':\n",
    "            a = relu(z)\n",
    "        else:\n",
    "            # No other functions supported at this time\n",
    "            pass\n",
    "        # Upload the results to ElastiCache for `TrainerLambda` to vectorize\n",
    "        to_cache(endpoint=endpoint, obj=a, name='layer'+str(layer)+'_a_'+str(ID))\n",
    "\n",
    "        # Debug Statement\n",
    "        #print(\"Completed Forward Propagation for epoch {}, layer {}\".format(str(epoch), str(layer)))\n",
    "        \n",
    "        if last == \"True\":\n",
    "            # Update parameters with this Neuron's data\n",
    "            parameters['epoch'] = epoch\n",
    "            parameters['layer'] = layer + 1\n",
    "            # Build the state payload\n",
    "            payload = {}\n",
    "            payload['parameter_key'] = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "            payload['state'] = 'forward'\n",
    "            payload['epoch'] = epoch\n",
    "            payload['layer'] = layer + 1\n",
    "\n",
    "            # Debug Statement\n",
    "            #print(\"Payload to be sent to TrainerLambda: \\n\" + dumps(payload, indent=4, sort_keys=True))\n",
    "            \n",
    "            trainer_handler(event=payload, context='')\n",
    "\n",
    "        return\n",
    "\n",
    "    elif state == 'backward':\n",
    "        # Backprop from Cost to X (A0)\n",
    "        \"\"\"\n",
    "        Intuition: TrainerLambda launched back prop with `layer-1`, therefore this should be \n",
    "        last \"active\" layer. That means that the \"dA\" for this layer has already been\n",
    "        calculate. Thus, no need to do the `A - Y` error calculation. Additionally, \n",
    "        the following code structure makes the it more idempotent for multiple layers.\n",
    "        \"\"\"\n",
    "        # Get necessary parameters\n",
    "        r = redis(host=endpoint, port=6379, db=15, charset=\"utf-8\", decode_responses=True)\n",
    "        z_key = []\n",
    "        for z in r.scan_iter(match='layer'+str(layer)+'_z_'+str(ID)+'*'):\n",
    "            z_key.append(z)\n",
    "        z = from_cache(endpoint=endpoint, key=z_key[0])\n",
    "        m = from_cache(endpoint=endpoint, key=parameters['data_keys']['m'])\n",
    "        A_prev = from_cache(endpoint=endpoint, key=parameters['data_keys']['A'+str(layer-1)])\n",
    "\n",
    "        # Get the derivative of the current layer's activation,\n",
    "        # based on the size of the layer.\n",
    "        if layer == parameters['layers']:\n",
    "            # If this is the last layer, then:\n",
    "            dA = from_cache(endpoint=endpoint, key=parameters['data_keys']['dA'+str(layer)])\n",
    "            W = from_cache(endpoint=endpoint, key=parameters['data_keys']['W'+str(layer)])\n",
    "        else:\n",
    "            dA = from_cache(endpoint=endpoint, key=parameters['data_keys']['dA'+str(layer)])[ID-1, :]\n",
    "            dA = dA.reshape(1, dA.shape[0])\n",
    "            W = from_cache(endpoint=endpoint, key=parameters['data_keys']['W'+str(layer)])[ID-1, :]\n",
    "            W = W.reshape(1, W.shape[0])\n",
    "        \n",
    "        # Calculate the derivative of the Activations\n",
    "        if activation=='sigmoid':\n",
    "            dZ = sigmoid_backward(dA, z)\n",
    "        elif activation == 'relu':\n",
    "            dZ = relu_backward(dA, z)\n",
    "        elif activaion == 'leaky_relu':\n",
    "            dZ = leaky_relu_backward(dA, z)\n",
    "        else:\n",
    "            # No other functions supported at this time\n",
    "            pass\n",
    "        # Upload the derivative of the activation to ElastiCache for use by `TrainerLambda`\n",
    "        to_cache(endpoint=endpoint, obj=dZ, name='layer'+str(layer)+'_dZ_'+str(ID))\n",
    "        \n",
    "        # Calculate the derivatives of the weights\n",
    "        dw = 1 / m * np.dot(dZ, A_prev.T)\n",
    "        # Upload the derivative of the weights to ElastiCache for use by `TrainerLambda`\n",
    "        to_cache(endpoint=endpoint, obj=dw, name='layer'+str(layer)+'_dw_'+str(ID))\n",
    "        \n",
    "        # Debug statement\n",
    "        assert(dw.shape == W.shape)\n",
    "\n",
    "        # Calculate the derivatives of the bias\n",
    "        db = 1 / m * np.sum(dZ, axis=1, keepdims=True) #<-- Could be an issue here and may have to reshape in TrainerLambda when runnign backprop oin layer 1\n",
    "        #db = 1 / m * np.sum(dZ)\n",
    "        # Upload the derivative of the bis to ElastiCache for use by `TrainerLambda`\n",
    "        to_cache(endpoint=endpoint, obj=db, name='layer'+str(layer)+'_db_'+str(ID))\n",
    "\n",
    "        # Debug Statement\n",
    "        #print(\"Completed Back Propogation for epoch {}, layer {}\".format(str(epoch), str(layer)))\n",
    "\n",
    "        if last == \"True\":\n",
    "            # Update parameters with this Neuron's data\n",
    "            parameters['epoch'] = epoch\n",
    "            parameters['layer'] = layer - 1\n",
    "            # Build the state payload\n",
    "            payload = {}\n",
    "            payload['parameter_key'] = to_cache(endpoint=endpoint, obj=parameters, name='parameters')\n",
    "            payload['state'] = 'backward'\n",
    "            payload['epoch'] = epoch\n",
    "            payload['layer'] = layer - 1\n",
    "\n",
    "            # Debug Statement\n",
    "            #print(\"Payload to be sent to TrainerLambda: \\n\" + dumps(payload, indent=4, sort_keys=True))\n",
    "            \n",
    "            trainer_handler(event=payload, context='')\n",
    "            \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Sample Workflow for a 2-Layers over 10 Epochs\n",
    "### Overview\n",
    "The following demonstrates the above code BLAH BLAH BLAH\n",
    "### Trigger Event from S3\n",
    "**Simulate the training data being uploaded to S3 and Launching the training process.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Neural Network Settings: \n",
      "\n",
      "{\n",
      "    \"activations\": {\n",
      "        \"layer1\": \"relu\",\n",
      "        \"layer2\": \"sigmoid\"\n",
      "    },\n",
      "    \"data_keys\": {\n",
      "        \"A0\": \"A0|float64#12288#209\",\n",
      "        \"W1\": \"W1|float64#3#12288\",\n",
      "        \"W2\": \"W2|float64#1#3\",\n",
      "        \"Y\": \"Y|int64#1#209\",\n",
      "        \"b1\": \"b1|float64#3#1\",\n",
      "        \"b2\": \"b2|float64#1#1\",\n",
      "        \"m\": \"m|int\",\n",
      "        \"results\": \"results|json\",\n",
      "        \"test_set_x\": \"test_set_x|float64#12288#50\",\n",
      "        \"test_set_y\": \"test_set_y|int64#1#50\",\n",
      "        \"train_set_x\": \"train_set_x|float64#12288#209\",\n",
      "        \"train_set_y\": \"train_set_y|int64#1#209\"\n",
      "    },\n",
      "    \"dims\": {\n",
      "        \"test_set_x\": [\n",
      "            12288,\n",
      "            50\n",
      "        ],\n",
      "        \"test_set_y\": [\n",
      "            1,\n",
      "            50\n",
      "        ],\n",
      "        \"train_set_x\": [\n",
      "            12288,\n",
      "            209\n",
      "        ],\n",
      "        \"train_set_y\": [\n",
      "            1,\n",
      "            209\n",
      "        ]\n",
      "    },\n",
      "    \"epochs\": 10,\n",
      "    \"input_data\": [\n",
      "        \"train_set_x\",\n",
      "        \"train_set_y\",\n",
      "        \"test_set_x\",\n",
      "        \"test_set_y\"\n",
      "    ],\n",
      "    \"layers\": 2,\n",
      "    \"learning_rate\": 0.0075,\n",
      "    \"neurons\": {\n",
      "        \"layer1\": 3,\n",
      "        \"layer2\": 1\n",
      "    },\n",
      "    \"s3_bucket\": \"itsacat-demo\"\n",
      "}\n",
      "Payload to be sent to TrainerLambda: \n",
      "{\n",
      "    \"parameter_key\": \"parameters|json\",\n",
      "    \"state\": \"start\"\n",
      "}\n",
      "Cost after epoch 0: 0.6827685092391731\n",
      "Cost after epoch 1: 0.6631341430192215\n",
      "Cost after epoch 2: 0.6523526592210159\n",
      "Cost after epoch 3: 0.6484447083861695\n",
      "Cost after epoch 4: 0.6464318397296804\n",
      "Cost after epoch 5: 0.6449335176599453\n",
      "Cost after epoch 6: 0.6435189072191083\n",
      "Cost after epoch 7: 0.6422075397475966\n",
      "Cost after epoch 8: 0.64091401869164\n",
      "Cost after epoch 9: 0.6396404752799577\n",
      "Training Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Simulate S3 event trigger data\n",
    "launch_handler(event, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJkAAAJcCAYAAABaP3UWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f705c057470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View the Results object stored on S3\n",
    "bucket = parameters['s3_bucket']\n",
    "content = s3_resource.Object(bucket, 'training_results/results.json')\n",
    "file = content.get()['Body'].read().decode('utf-8')\n",
    "json_content = json.loads(file)\n",
    "#print(json_content)\n",
    "costs = []\n",
    "for k, v in json_content.items():\n",
    "    costs.append(v.get('cost'))\n",
    "plt.plot(costs)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title(\"Learning Rate: \" + str(parameters['learning_rate']))\n",
    "plt.rcParams['figure.figsize'] = (20, 10)\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Step: Training the Classifier\n",
    "Please refer to the [**README**](../README.md) on how to start training the neural network model and then [**Analyze**](./Analysis.ipynb) the results."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XaIWT",
   "launcher_item_id": "zAgPl"
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
