{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Model Training and Prediction - Built-in Image Classification Model\n",
    "## Introduction\n",
    "[Amazon SageMaker](https://aws.amazon.com/sagemaker/?sc_channel=PS&sc_campaign=pac_ps_q4&sc_publisher=google&sc_medium=sagemaker_b_pac_search&sc_content=sagemaker_e&sc_detail=aws%20sagemaker&sc_category=sagemaker&sc_segment=webp&sc_matchtype=e&sc_country=US&sc_geo=namer&sc_outcome=pac&s_kwcid=AL!4422!3!245225393502!e!!g!!aws%20sagemaker&ef_id=WL2I0wAAAIRC8xLB:20180418161912:s) is a fully mamnaged platform that enables Data Scientists to build, train and deploy machine learning models at any scale. It provides key services necessary to create and manage a Machine Learning (ML) Pipeline from \"Notebook\" to \"Production\", as highlighted below:\n",
    "\n",
    "<img src=\"images/SageMaker_Workflow.png\" style=\"width:800px;height:200px;\">\n",
    "\n",
    "The following Notebook demonstrates this process by using SageMaker's built-in [Image Classification Algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html). To accomplish this, SageMaker's Image Classification Algorithm leverages a commonly used and pre-built model for image classification called __Resnet__. You can read more about Resnet [here](https://arxiv.org/abs/1512.03385).\n",
    "\n",
    "<img src=\"images/CNN.jpg\" style=\"width:950px;height:400px;\">\n",
    "<caption><left>[*image source](https://www.MathWorks.com)</left></caption><br>\n",
    "\n",
    "By leveraging this methodology, the Data Scientist or Developer doesn't need to expend time to build, train and optimize a custom Image Classification models, but rather simply provide the training data and left SageMaker perform all the heavy lifting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "import h5py\n",
    "import json\n",
    "import tarfile\n",
    "import datetime\n",
    "import urllib.request\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from time import gmtime, strftime\n",
    "from IPython.display import Image\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "%matplotlib inline\n",
    "\n",
    "# Configure SageMaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "training_image = get_image_uri(boto3.Session().region_name, 'image-classification') #Image Classification Estimator\n",
    "\n",
    "# Helper Functions\n",
    "def make_lst(data, label, name):\n",
    "    \"\"\"\n",
    "    Make a custom tab separated `lst` file for `im2rec.py`\n",
    "    \n",
    "    Arguments:\n",
    "    data -- Numpy array of image data\n",
    "    label -- \"Truth\" label for image classification\n",
    "    name -- \"train\" or \"test\" data\n",
    "    \"\"\"\n",
    "    # Create local repository for the images based on name\n",
    "    if not os.path.exists('./'+name):\n",
    "        os.mkdir('./'+name)\n",
    "        \n",
    "    # Create the `.lst` file\n",
    "    lst_file = './'+name+'.lst'\n",
    "    \n",
    "    # Iterate through the numpy arrays and save as `.jpg`\n",
    "    # and update the `.lst` file\n",
    "    for i in range(len(data)):\n",
    "        img = data[i]\n",
    "        img_name = name+'/'+str(i)+'.jpg'\n",
    "        imageio.imwrite(img_name, img)\n",
    "        with open(lst_file, 'a') as f:\n",
    "            f.write(\"{}\\t{}\\t{}\\n\".format(str(i), str(label[i]), img_name))\n",
    "            f.flush()\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Overview\n",
    "## Input Data Preparation\n",
    "To train the Neural Network, we are provided with a dataset (`datasets.h5`) containing:\n",
    "- a training set of $m$ images containing cats and non-cats as well as the appropriate class labels ($y=1$) and non-cat images ($y=0$).\n",
    "- a test set of $m$ images containing cats and non-cat as well as the appropriate class labels ($y=1$) and non-cat images ($y=0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Training and Testing dataset\n",
    "dataset = h5py.File('datasets/datasets.h5', 'r')\n",
    "\n",
    "# Createw the Training and Testing data sets\n",
    "X_train = np.array(dataset['train_set_x'][:])\n",
    "y_train = np.array(dataset['train_set_y'][:])\n",
    "X_test = np.array(dataset['test_set_x'][:])\n",
    "y_test = np.array(dataset['test_set_y'][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the cell above, the image training and testing (validation) input data (`X_train` and `X_test`) are 4-dimensional arrays consisting of $209$ training examples ($m$) and $50$ testing images. Each image is in turn of height, width and depth (__R__ed, __G__reen __B__lue values) of $64 \\times 64 \\times 3$. Additionally, the dimension for the \"true\" labels (`y_train` and `y_test`) only show a $209$ and $50$ column structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data set dimensions\n",
    "print(\"Training Data Dimension: {}\".format(X_train.shape))\n",
    "print(\"No. Training Examples: {}\".format(X_train.shape[0]))\n",
    "print(\"No. Training Features: {}\".format(X_train.reshape((-1, 12288)).shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data set dimensions\n",
    "print(\"Training Data Dimension: {}\".format(X_test.shape))\n",
    "print(\"No. Training Examples: {}\".format(X_test.shape[0]))\n",
    "print(\"No. Training Features: {}\".format(X_test.reshape((-1, 12288)).shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SageMaker's built-in Image Classification algorithm works best when the training dataset is optimizied for protobuf [RecordIO](https://mxnet.incubator.apache.org/architecture/note_data_loading.html) format. RecordIO (__content type:__ application/x-recordio) is an efficient file format that feeds images for model training as a stream, thus allowing for the entire dataset to be loaded either into CPU or GPU memory and thus vastly iomproving the model training time. . Some fo the benefits include:\n",
    "- Storing images in a compact format, which greatly reduces the size of the dataset on the disk.\n",
    "- Packing data together allows continuous reading on the disk.\n",
    "- RecordIO has a simple way to partition, simplifying the distriburtion of training data when leveraging distributed training.\n",
    "\n",
    "Optimizing the image data for protobuf RecordIO format, requires the numpy arrays for training and testing to be converted to this format, uploaded to S3 and then streamed to the training instance memory. This process is referred to as *Pipe mode* in SageMaker and the MXNet community provides a tool, [im2rec](https://raw.githubusercontent.com/apache/incubator-mxnet/master/tools/im2rec.py), that helps to convert images to RecordIO format. More information can be found in the [tutorial](https://mxnet.incubator.apache.org/faq/recordio.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of this notebook, where the native images (__content type:__ application/x-image) will be used, instead of RecordIO. This process is referred to as *File mode* in SageMaker. Since the Training and Testing data sets are currently stored as Numpy arrays, they will need to be converted to native `.jpg` images before uploading them to S3.\n",
    "\n",
    "SageMaker's built-in image classifier has the capability to convert the native images to RecordIO format as part of the trianing function. However, the associated metadata (i.e. the classification label of the image) needs to be captured along with the image when uploading to S3. In order to accomplish this, a `.lst` file needs to be created that captures the metadata of the image and it's associated label.\n",
    "\n",
    "An `.lst` file is a tab-separated file with three columns that contains a list of image files. The first column specifies the image index, the second column specifies the class label index for the image, and the third column specifies the relative path of the image file. The image index in the first column should be unique across all of the images. The following code cell leverages the `make_lst()` helper function to accomplish this and sows an example of what the `.lst` file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numpy arrays to images and create `lst` file\n",
    "make_lst(X_train, y_train, name='train') # Training data\n",
    "make_lst(X_test, y_test, name='test') # Testing data\n",
    "# View the output of the training `.lst` file\n",
    "print(\"Sample output of the `train.lst` file:\\n\")\n",
    "!head -n 3 ./train.lst > example.lst\n",
    "f = open('example.lst','r')\n",
    "lst_content = f.read()\n",
    "print(lst_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample image from file\n",
    "from PIL import Image\n",
    "image = mpimg.imread('./train/2.jpg')\n",
    "plt.imshow(image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data Upload\n",
    "\n",
    "In order for SageMaker to execute the training and validation process on the Input Data, the data needs to be uploaded to S3. SageMaker provides the handy function, upload_data(), to upload the Numpy data to a default (or specific) location. If not already created, the function will create an S3 bucket. The resulting S3 bucket will also store the various training and testing output that will be used for creating production Endpoints and Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the Training and Testing Data to S3\n",
    "training_data = sagemaker_session.upload_data(path='./train', key_prefix='train')\n",
    "training_lst = sagemaker_session.upload_data(path='./train.lst', key_prefix='train_lst')\n",
    "test_data = sagemaker_session.upload_data(path='./test', key_prefix='test')\n",
    "test_lst = sagemaker_session.upload_data(path='./test.lst', key_prefix='test_lst')\n",
    "bucket = training_data.split('/')[2]\n",
    "print(\"S3 Bucket: {}\".format(bucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Training the SageMaker Classifier\n",
    "## Hyperparameters\n",
    "There are two kinds of parameters that need to be set for training. The first one are the hyperparameters that are specific to the algorithm. These are:\n",
    "\n",
    "- __num_layers:__ The number of layers (depth) for the network. We use `18` in this example due to the fact that the training images are smalle ($64\\times64$). More layers are typically used for much larger training images.\n",
    "- __image_shape:__ The input image dimensions,'num_channels, height, width', for the network. It should be no larger than the actual image size. The number of channels should be same as the actual image.\n",
    "    <div class=\"alert alert-info\">\n",
    "      <strong>Info!</strong> The original image data is shaped as <b>64 x 64 x 3</b>. RecordIO prefers the data formatted with channels first, i.e. <b>3 x 64 x 64</b>. Since the built-in image classifier will automatically convert the native images to RecordIO format before executing the training, the `image_shape` hyperparameter dimensions are the dimensions of the data after becing converted to RecordIO, i.e. <b>3 x 63 x 63</b>.\n",
    "    </div>\n",
    "- __num_training_samples:__ This is the total number of training examples. It is set to $209$.\n",
    "- __num_classes:__ This is the number of output classes for the new dataset. Imagenet was trained with 1000 output classes but the number of output classes can be changed for fine-tuning. For this training set, $2$ is used because it has $2$ object categories, __cat__ or __non-cat__.\n",
    "- __mini_batch_size:__ The number of training samples used for each mini batch. In distributed training, the number of training samples used per batch will be $N \\times mini_batch_size$ where $N$ is the number of hosts on which training is run.\n",
    "- __epochs:__ Number of training epochs.\n",
    "- __learning_rate:__ Learning rate for training.\n",
    "- __use_pretrained_model:__ Set to $0$ since the example will not be using transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm supports multiple network depth (number of layers). They are 18, 34, 50, 101, 152 and 200\n",
    "num_layers = 18\n",
    "\n",
    "# Shape of the training images\n",
    "image_shape = \"3,64,64\"\n",
    "\n",
    "# No. Samples in Training set\n",
    "num_training_samples = 209\n",
    "\n",
    "# No. output classes\n",
    "num_classes = 2\n",
    "\n",
    "# Batch Size\n",
    "mini_batch_size =  42\n",
    "\n",
    "# No. Epochs\n",
    "epochs = 6\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Since transfer learning is not used, set use_pretrained_model to `0`\n",
    "# so that weights can be initialized WITHOUT pre-trained weights\n",
    "use_pretrained_model = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second set of parameters, are those that are specific to the SageMaker training job. These include:\n",
    "\n",
    "- __Input specification:__ These are the training and validation channels that specify the path where training data is present. These are specified in the `InputDataConfig` section. The main parameters that need to be set is the `ContentType` which can be set to *application/x-image* since the input data format and the `S3Uri` specifies the bucket and the folder where the training images are stored.\n",
    "- __Output specification:__ This is specified in the `OutputDataConfig` section, the path to where the output can be stored after training. \n",
    "- __Resource config:__ This section specifies the type of instance on which to run the training and the number of hosts used for training. If `InstanceCount` is more than $1$, then training can be run in a distributed manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique job name \n",
    "job_name_prefix = 'sagemaker-imageclassification'\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "job_name = job_name_prefix + timestamp\n",
    "training_params = \\\n",
    "{\n",
    "    # Specify the training docker image\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": training_image,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": 's3://{}/{}/output'.format(bucket, job_name_prefix)\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,\n",
    "        \"InstanceType\": \"ml.p2.xlarge\", # GPU Instance\n",
    "        \"VolumeSizeInGB\": 50\n",
    "    },\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": {\n",
    "        \"image_shape\": image_shape,\n",
    "        \"num_layers\": str(num_layers),\n",
    "        \"num_training_samples\": str(num_training_samples),\n",
    "        \"num_classes\": str(num_classes),\n",
    "        \"mini_batch_size\": str(mini_batch_size),\n",
    "        \"epochs\": str(epochs),\n",
    "        \"learning_rate\": str(learning_rate),\n",
    "        \"use_pretrained_model\": str(use_pretrained_model)    \n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 360000\n",
    "    },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\", # Training Images Location\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": 's3://{}/'.format(bucket),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-image\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\", # Testing Images Location\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": 's3://{}/'.format(bucket),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-image\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"train_lst\", # Image Metadata\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": 's3://{}/train_lst/'.format(bucket),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-image\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation_lst\", # Image metadata\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": 's3://{}/test_lst/'.format(bucket),\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-image\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "print('Training job name: {}'.format(job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Job\n",
    "Use the SageMaker `create_training_job()` method to start the training with the above parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Amazon SageMaker training job\n",
    "sagemaker = boto3.client(service_name='sagemaker')\n",
    "sagemaker.create_training_job(**training_params)\n",
    "\n",
    "# confirm that the training job has started\n",
    "status = sagemaker.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print('Training job current status: {}'.format(status))\n",
    "\n",
    "try:\n",
    "    # wait for the job to finish and report the ending status\n",
    "    sagemaker.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=job_name)\n",
    "    training_info = sagemaker.describe_training_job(TrainingJobName=job_name)\n",
    "    status = training_info['TrainingJobStatus']\n",
    "    print(\"Training job ended with status: \" + status)\n",
    "except:\n",
    "    print('Training failed to start')\n",
    "     # if exception is raised, that means it has failed\n",
    "    message = sagemaker.describe_training_job(TrainingJobName=job_name)['FailureReason']\n",
    "    print('Training failed with the following error: {}'.format(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Hosting the SageMaker Model\n",
    "Now that the image classification model has been trained, it can be used to perform inferences, i.e. is the picture a \"cat\" or \"non-cat\" picture. This section involves the following steps:\n",
    "- __Create model:__ Create model for the training output.\n",
    "- __Create endpoint configuration:__ Create a configuration defining an endpoint.\n",
    "- __Create endpoint:__ Use the configuration to create an inference endpoint.\n",
    "- __Prediction:__ Perform inference on some input data using the endpoint.\n",
    "\n",
    "## Create Model\n",
    "Use the model created as output from training, to create the Endpoint Configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hosting model\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "model_name = \"image-classification\" + timestamp\n",
    "info = sagemaker.describe_training_job(TrainingJobName=job_name)\n",
    "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(\"Model mame: {}\".format(model_name))\n",
    "\n",
    "# SageMaker Hossting Image\n",
    "hosting_image = get_image_uri(boto3.Session().region_name, 'image-classification')\n",
    "primary_container = {\n",
    "    'Image': hosting_image,\n",
    "    'ModelDataUrl': model_data,\n",
    "}\n",
    "\n",
    "# Create the Model\n",
    "create_model_response = sagemaker.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Endpoint Configuration\n",
    "Next, configure __REST__ endpoints for hosting multiple models, e.g. for __A/B__ testing purposes. In order to support this, create an endpoint configuration, that describes the distribution of traffic across the models, whether split, shadowed, or sampled in some way. In addition, the endpoint configuration describes the instance type required for model deployment, and at launch will describe the autoscaling configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Endpoint configuration\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_config_name = job_name_prefix + '-config' + timestamp\n",
    "endpoint_config_response = sagemaker.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants = [\n",
    "        {\n",
    "            'InstanceType':'ml.m5.xlarge', # Non-GPU Instance for hosting\n",
    "            'InitialInstanceCount':1,\n",
    "            'ModelName':model_name,\n",
    "            'VariantName':'AllTraffic'\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print('Endpoint configuration name: {}'.format(endpoint_config_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Hosting Endpoint\n",
    "Through specifying the name and configuration defined above an endpoint is created that will be used to predictions on un-seen data as well as incorporated into production applications. This takes 9-11 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Endpoint\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_name = job_name_prefix + '-endpoint' + timestamp\n",
    "print('Endpoint name: {}'.format(endpoint_name))\n",
    "\n",
    "endpoint_params = {\n",
    "    'EndpointName': endpoint_name,\n",
    "    'EndpointConfigName': endpoint_config_name,\n",
    "}\n",
    "endpoint_response = sagemaker.create_endpoint(**endpoint_params)\n",
    "\n",
    "# Get the status of the endpoint\n",
    "response = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = response['EndpointStatus']\n",
    "print('EndpointStatus = {}'.format(status))\n",
    "    \n",
    "try:\n",
    "    sagemaker.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
    "finally:\n",
    "    resp = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Create endpoint ended with status: \" + status)\n",
    "    print(\"Ednpoint Arn: \" + resp['EndpointArn'])\n",
    "    \n",
    "    if status != 'InService':\n",
    "        message = sagemaker.describe_endpoint(EndpointName=endpoint_name)['FailureReason']\n",
    "        print('Training failed with the following error: {}'.format(message))\n",
    "        raise Exception('Endpoint creation did not succeed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Prediction\n",
    "\n",
    "Finally, the endpoint can be obtained from the client library using the result from previous operations, and generate classifications from the trained model using that endpoint to predict whether a new (previously unseen) image is classified as a **cat** or **non-cat** image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(20.0,20.0))\n",
    "columns = 2\n",
    "threshold = 0.5\n",
    "\n",
    "# Create label Classes\n",
    "classes = [\"non-cat\", \"cat\"]\n",
    "\n",
    "# Inference client\n",
    "runtime = boto3.Session().client(service_name='runtime.sagemaker')\n",
    "\n",
    "# Get Image files\n",
    "images = []\n",
    "for img_path in glob.glob('./images/*.jpeg'):\n",
    "    images.append(img_path)\n",
    "\n",
    "# Run each image against the Inference Endpoint and plot results\n",
    "for i, image in enumerate(images):\n",
    "    with open(image, 'rb') as f:\n",
    "        payload = f.read()\n",
    "        payload = bytearray(payload)\n",
    "    response = runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='application/x-image',\n",
    "        Body=payload\n",
    "    )\n",
    "    result = json.loads(response['Body'].read())\n",
    "    if result[0] > threshold:\n",
    "        prediction = classes[1]\n",
    "    else:\n",
    "        prediction = classes[0]\n",
    "    plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
    "    plt.title('Prediction = \"{}\" picture.'.format(prediction))\n",
    "    plt.imshow(mpimg.imread(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: Test the Production API\n",
    "Now that the model has been trained and validated for production, the **Data Science** part of the ML Pipeline can be integrated into the **DevOps** process. Refer back to the [README](../README.md) on the next steps.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "  <strong>Note:</strong> Make sure to remember the name of the training job, as it is necessary to complete the next steps.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
