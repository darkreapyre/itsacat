{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Model Training and Prediction\n",
    "## Introduction\n",
    "[Amazon SageMaker](https://aws.amazon.com/sagemaker/?sc_channel=PS&sc_campaign=pac_ps_q4&sc_publisher=google&sc_medium=sagemaker_b_pac_search&sc_content=sagemaker_e&sc_detail=aws%20sagemaker&sc_category=sagemaker&sc_segment=webp&sc_matchtype=e&sc_country=US&sc_geo=namer&sc_outcome=pac&s_kwcid=AL!4422!3!245225393502!e!!g!!aws%20sagemaker&ef_id=WL2I0wAAAIRC8xLB:20180418161912:s) is a fully mamnaged platform that enables Data Scientists to build, train and deploy machine learning models at any scale. It provides key services necessary to create and manage a Machine Learning (ML) Pipeline from \"Notebook\" to \"Production\", as highlighted below:\n",
    "\n",
    "<img src=\"images/SageMaker_Workflow.png\" style=\"width:800px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1 - Notebook Instance\n",
    "__Using the Notebook instance to understand the Data__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permissions and Environmental Variables\n",
    "\n",
    "The packages that will be needed to prepare the data and train the model are as follows:\n",
    "- [datetime](https://docs.python.org/2/library/datetime.html) provides classes for manipulating dates and times in both simple and complex ways.\n",
    "- [numpy](https://www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [matplotlib](https://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "- [PIL](http://www.pythonware.com/products/pil/) is used here to test the model on unseen image data at the end.\n",
    "- [boto3](https://pypi.python.org/pypi/boto3) is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python, which allows Python developers to write software that makes use of services like Amazon S3 and Amazon EC2.\n",
    "- [json](https://docs.python.org/3/library/json.html) is a lightweight data interchange format inspired by JavaScript object literal syntax (although it is not a strict subset of JavaScript.\n",
    "- [os](https://docs.python.org/3/library/os.html) is a module the provides a portable way of using operating system dependent functionality. Particularly the  environ object is a mapping object representing the environment.\n",
    "- [tarfile](https://docs.python.org/3/library/tarfile.html) is used to read and write tar archives, when extracting the model training results from S3.\n",
    "- [mxnet](http://mxnet.incubator.apache.org) is a flexable and effecient library for deep learning.\n",
    "- [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk) is an open source library for training and deploying machine learning models on Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import h5py\n",
    "import json\n",
    "import tarfile\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from sagemaker.mxnet import MXNet\n",
    "from mxnet import gluon\n",
    "%matplotlib inline\n",
    "\n",
    "# Configure SageMaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data Preparation\n",
    "To train the Neural Network, we are provided with a dataset (`datasets.h5`) containing:\n",
    "- a training set of $m$ images containing cats and non-cats as well as the appropriate class labels ($y=1$) and non-cat images ($y=0$).\n",
    "- a test set of $m$ images containing cats and non-cat as well as the appropriate class labels ($y=1$) and non-cat images ($y=0$).\n",
    "\n",
    ">**Note:** The original dataset was comprised of two separate files, `test_catvnoncat.h5` and `train_catvnoncat.h5`. For the sake of this implementation a single file is used, `datasets.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local repository for Numpy Arrays\n",
    "if not os.path.exists('/tmp/data'):\n",
    "    os.mkdir('/tmp/data')\n",
    "\n",
    "# Load the Training and Testing dataset\n",
    "dataset = h5py.File('datasets/datasets.h5', 'r')\n",
    "\n",
    "# Save the Dataset as Numpy Arrays\n",
    "np.save('/tmp/data/train_X.npy', np.array(dataset['train_set_x'][:]))\n",
    "np.save('/tmp/data/train_Y.npy', np.array(dataset['train_set_y'][:]))\n",
    "np.save('/tmp/data/test_X.npy', np.array(dataset['test_set_x'][:]))\n",
    "np.save('/tmp/data/test_Y.npy', np.array(dataset['test_set_y'][:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From the cell above, the image training and testing (validation) input data (`train_set_x` and `test_set_x`) are 4-dimensional arrays consiting of $209$ training examoples ($m$) and $50$ testing images. Each image is in turn of height, width and depth (**R**ed, **G**reen **B**lue values) of $64 \\times 64 \\times 3$. Additionally, the dimension for the \"true\" labels (`train_set_y` and `test_set_y`) only show a $209$ and $50$ column structure.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data set dimensions\n",
    "print(\"Training Data Dimension: {}\".format(np.load('/tmp/data/train_X.npy').shape))\n",
    "print(\"No. Training Examples: {}\".format(np.load('/tmp/data/train_X.npy').reshape((-1, 12288)).shape[0]))\n",
    "print(\"No. Training Features: {}\".format(np.load('/tmp/data/train_X.npy').reshape((-1, 12288)).shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data set dimensions\n",
    "print(\"Testing Data Dimension: {}\".format(np.load('/tmp/data/test_X.npy').shape))\n",
    "print(\"No. Testing Examples: {}\".format(np.load('/tmp/data/test_X.npy').reshape((-1, 12288)).shape[0]))\n",
    "print(\"No. Testing Features: {}\".format(np.load('/tmp/data/test_X.npy').reshape((-1, 12288)).shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The following code cell shows the *observation label* and its corresponding *image*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a cat picture\n",
    "plt.imshow(np.load('/tmp/data/train_X.npy')[2]);\n",
    "print(\"y = {}, and therefore it's a \\\"cat\\\" picture.\".format(np.load('/tmp/data/train_Y.npy')[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data Upload\n",
    "In order for *SageMaker* to execute the training and validation process on the Input Data, the data needs to be uploaded to S3. *SageMaker* provides the handy function, `upload_data()`, to upload the Numpy data to a default (or specific) location. If not already created, the function will create an S3 bucket. The resulting S3 bucket will also store the various training and testing output that will be used for creating production *Endpoints* and *Analysis*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the Training and Testing Data to S3\n",
    "inputs = sagemaker_session.upload_data(path='/tmp/data', key_prefix='training_input')\n",
    "bucket = inputs.split('/')[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 - Model Training Jobsl\n",
    "### Training Function\n",
    "The *Training Function*, `model.py`, contains the instructions that *SageMaker* needs to:\n",
    "1. Load the Input training and validation data sets from S3; `get_data()`.\n",
    "2. Pre-process, \"vectorize\" and scale the image data to be processed by the Neural Network; `transform()`.\n",
    "3. Train the model and validate the prediction accuracy of the proposed Neural Network model on the Input data; `train()`.\n",
    "4. Save the model and training results to S3; `save()`.\n",
    "\n",
    "Below is a graphical representation of the Neural Network model implemented in the *Training Function*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from model import create_graph\n",
    "net = create_graph()\n",
    "net.collect_params().initialize()\n",
    "x = mx.sym.var('data')\n",
    "sym = net(x)\n",
    "mx.viz.plot_network(sym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For an overview of the model function code, refer to [Appendix A](#Appendix-A:-Image-Classification-Model).\n",
    "\n",
    "### MXNet Estimator\n",
    "*SageMaker* provides built-in functionality to train and host [MXNet](http://mxnet.incubator.apache.org) and [Gluon](http://gluon.mxnet.io) models, using the `MXNet` class of the [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk). Leveraging the MXNet Estimator drastically simplifies the handling of end-to-end training as well as deployment of custom MXNet models.\n",
    "\n",
    "Using the code (below), the model itself, the location of the training data and the Hyperparameters are applied to the MXNet Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MXNet Estimator\n",
    "mxnet_estimator = MXNet(\n",
    "    'model.py',\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.xlarge',\n",
    "    output_path='s3://'+bucket,\n",
    "    hyperparameters={\n",
    "        'epochs': 3000,\n",
    "        'optmizer': 'sgd',\n",
    "        'learning_rate': 0.0075,\n",
    "        'batch_size': 64\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Job\n",
    "By calling the estimator's `fit()` method, with the location of the training data, *SageMaker* can start the model training using the configuration provided. After the training is successfully completed, the training results can be analyzed. Should the results prove that the model is optimal, It can then be deployed to *SageMaker's* hosting services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Job name for current training run\n",
    "#job_name = '<<Specific Training Job Name>>'\n",
    "#mxnet_estimator.fit(inputs, job_name=job_name) # Fit the estimator to custom job name\n",
    "mxnet_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## 2 - Model Analysis\n",
    "After a model has been trained and before it can be leveraged in production, it must be tested. This testing process typically takes the form of:\n",
    "1. **Analyzing the results from the training process:**\n",
    "    A good indication that the model performs well on the training data is to verify that the overall Training Error (Cost Function) decreases after every iteration of the forward propagation process.\n",
    "2. **Classification Accuracy (Training data set):**\n",
    "    While the Training Error provides a good indication of how well the Neural Network out probabilities agree with the observed labels, a common evaluation metric used for classification models sn the **Accuracy Score**. This metric generally summarizes the number of correct predictions the classifier has made as a ratio of all the predictions.\n",
    "3. **Classification Accuracy (Test/Validation data set):**\n",
    "    A good practice in machine learning is to create a subset of the training data keep it separate for testing. This is typically referred to as a hold-out, validation or test set. By testing how well the model performance against this data, further insight can be derived.\n",
    "    \n",
    "As can be seen from the output from training process above, the model learn the features of the training set to accurately classify the observed label. Additionally, when the model applies the optimized parameters to classify the test data, it achieves as overall accuracy of $80%$.\n",
    "\n",
    "Since the training function also captures these results to S3, the Cost/Error, training set Accuracy and test set Accuracy can be visualized as follows:\n",
    ">**Note**: Be sure to enter the name of the above SageMaker training job in `job_name` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and uncompress output results from model training\n",
    "#job_name = '<<Enter Training Job Name>>'\n",
    "job_name = 'sagemaker-mxnet-2018-04-13-22-58-56-131'\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(bucket).download_file(job_name+'/output/output.tar.gz', '/tmp/output.tar.gz')\n",
    "tarfile.open('/tmp/output.tar.gz').extractall()\n",
    "with open('results.json') as j:\n",
    "    data = json.load(j)#, object_pairs_hook=OrderedDict)\n",
    "\n",
    "# Format data for plotting\n",
    "costs = []\n",
    "val_acc = []\n",
    "train_acc = []\n",
    "for key, value in sorted(data.items()):#, key=lambda (k,v): (v, k)):\n",
    "    if 'epoch' in key:\n",
    "        for k, v in value.items():\n",
    "            if k == 'cost':\n",
    "                costs.append(v)\n",
    "            elif k == 'val_acc':\n",
    "                val_acc.append(v)\n",
    "            elif k == 'train_acc':\n",
    "                train_acc.append(v)\n",
    "    elif 'Start' in key:\n",
    "        start = datetime.datetime.strptime(value, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    elif 'End' in key:\n",
    "        end = datetime.datetime.strptime(value, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "val_acc = np.array(val_acc)\n",
    "train_acc = np.array(train_acc)\n",
    "costs = np.array(costs)\n",
    "delta = end - start\n",
    "print(\"Model Training Time: {} Minute(s)\".format(int(delta.total_seconds() / 60)))\n",
    "\n",
    "# Plot the Learning Curve\n",
    "plt.rcParams['figure.figsize'] = (11.0, 10.0)\n",
    "plt.grid(True, which='both')\n",
    "plt.plot(costs)\n",
    "plt.plot(train_acc)\n",
    "plt.plot(val_acc)\n",
    "plt.ylabel('Cost / Accuracy')\n",
    "plt.xlabel('Epochs (in Hundreds)')\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend(['Cost', 'Training Accuracy', 'Validation Accuracy'])\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## 3 - Prediction Analysis using Inference Endpoints\n",
    "Testing the model against an image that is neither part of the training data or the testing data will provide realistic proof of it's performance in production. The following code cells demonstrate how the trained model performs against a selection of images that have pictures of cats as well as \"other\" pictures.\n",
    "\n",
    "To further simulate the predictive capabilities of the trained mode in a production environment, the `deploy()` method of the estimator is called to host the model on the *SageMaker* [hosting services](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html) which provides an HTTPS endpoint to provide classification inferences on images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = mxnet_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cells below show the pseudo production classification inferences on unseen image data by leveraging the hosted predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.image as mpimg\n",
    "from skimage import transform\n",
    "\n",
    "# Get Classes\n",
    "classes = [\"non-cat\", \"cat\"]\n",
    "\n",
    "# Get Image files\n",
    "images = []\n",
    "for img_path in glob.glob('./images/*.jpeg'):\n",
    "    images.append(mpimg.imread(img_path))\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(20.0,20.0))\n",
    "columns = 2\n",
    "for i, image in enumerate(images):\n",
    "    img = transform.resize(image, (64, 64), mode='constant').reshape((1, 64 * 64 * 3))\n",
    "    prediction = int(predictor.predict(img.tolist()))\n",
    "    plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
    "    plt.title('Prediction = \"{}\" picture.'.format(classes[prediction]))\n",
    "    plt.imshow(image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleanup-up and Next Steps\n",
    "Delete the hosted endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxnet_estimator.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Appendix A: Image Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!cat model.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
